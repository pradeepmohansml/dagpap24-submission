{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1550fd68-aa39-44ea-b3cd-0ef9597d6af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dagpap24_cs_roberta import main as cs_robertamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b428025b-21ad-4a78-9b44-7b0e68e2166f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n",
      "2024-04-29 18:49:06,698 - INFO - Entering main function and starting argument parser\n",
      "2024-04-29 18:49:06,699 - INFO - Got project Root as : /home/ec2-user/SageMaker/dagpap24-submission/notebooks\n",
      "2024-04-29 18:49:06,703 - INFO - Loading train and test datasets\n",
      "2024-04-29 18:49:06,703 - INFO - Loading train dataset from file\n",
      "2024-04-29 18:49:11,222 - INFO - Initial train data length: 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d6263a104c4c93bb8d3f9fea6a9401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 18:49:14,895 - INFO - train data length after chunking to max 512 tokens: 56929\n",
      "2024-04-29 18:49:14,896 - INFO - Loading test dataset from file\n",
      "2024-04-29 18:49:19,091 - INFO - Initial test data length: 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de73c8c7c7b4e8a9b7763f57fb508c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 18:49:23,272 - INFO - test data length after chunking to max 512 tokens: 56929\n",
      "2024-04-29 18:49:23,273 - INFO - Splitting train data into train and validation splits\n",
      "2024-04-29 18:49:23,281 - INFO - Final train size: 45543\n",
      "2024-04-29 18:49:23,282 - INFO - Final validation size: 11386\n",
      "2024-04-29 18:49:23,282 - INFO - Final test size: 56929\n",
      "2024-04-29 18:49:23,283 - INFO - Writing train df to json...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9410f1022a2448bfaed0b6ad769d65e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 18:49:34,998 - INFO - Writing val df to json...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d4b84a3f2d4d77b517e0d88c6346de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 18:49:37,364 - INFO - Writing test df to json...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c899889a92a4254b4c2d226e6ae6d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 18:49:53,462 - INFO - Checkpoint detected, resuming training at /home/ec2-user/SageMaker/dagpap24-submission/notebooks/data/output_dev_cs_roberta/checkpoint-55000.                     To avoid this behavior, change the `--output_dir` or                     add `--overwrite_output_dir` to train from scratch.\n",
      "2024-04-29 18:49:53,464 - WARNING - Process rank: 0,             device: cuda:0, n_gpu: 1distributed training: True,             16-bits training: False\n",
      "2024-04-29 18:49:53,464 - INFO - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=5,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=error,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/ec2-user/SageMaker/dagpap24-submission/notebooks/data/output_dev_cs_roberta/runs/Apr29_18-49-53_ip-172-16-37-122.ec2.internal,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/home/ec2-user/SageMaker/dagpap24-submission/notebooks/data/output_dev_cs_roberta,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/ec2-user/SageMaker/dagpap24-submission/notebooks/data/output_dev_cs_roberta,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=5000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=0,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e1ea05a10b46f8a4c4882049f53234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593b089d3d094142aac88d9e40361045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe407efd2064e4686beb2b818271d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3258b40d20a40acb87b31f46dd9a140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c888127fd2f54740a47780e6cb87f17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:728] 2024-04-29 18:50:40,246 >> loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--allenai--cs_roberta_base/snapshots/11a2149cfda93e7e42aa076c1c9b6d244e237c1b/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-04-29 18:50:40,251 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"allenai/cs_roberta_base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-04-29 18:50:40,266 >> loading file vocab.json from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-04-29 18:50:40,267 >> loading file merges.txt from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-04-29 18:50:40,267 >> loading file tokenizer.json from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-04-29 18:50:40,268 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-04-29 18:50:40,268 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-04-29 18:50:40,268 >> loading file tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:728] 2024-04-29 18:50:40,269 >> loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-04-29 18:50:40,271 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3257] 2024-04-29 18:50:40,377 >> loading weights file pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/hub/models--allenai--cs_roberta_base/snapshots/11a2149cfda93e7e42aa076c1c9b6d244e237c1b/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:3982] 2024-04-29 18:50:40,676 >> Some weights of the model checkpoint at allenai/cs_roberta_base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3994] 2024-04-29 18:50:40,677 >> Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at allenai/cs_roberta_base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62b8d97ba234d49a8caae37f74758f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/45543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bb2db507384e99b1ce62e248ea3ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/11386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fe218d0c634f25b9ea7439972c6a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/56929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56930' max='56930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56930/56930 23:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 19:14:43,264 - INFO - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        10.0\n",
      "  total_flos               = 110831608GF\n",
      "  train_loss               =         0.0\n",
      "  train_runtime            =  0:23:23.66\n",
      "  train_samples            =       45543\n",
      "  train_samples_per_second =     324.458\n",
      "  train_steps_per_second   =      40.558\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 19:20:46,149 - INFO - *** Predict ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_f1                 =     0.9949\n",
      "  eval_loss               =     0.0217\n",
      "  eval_runtime            = 0:06:02.87\n",
      "  eval_samples            =      11386\n",
      "  eval_samples_per_second =     31.377\n",
      "  eval_steps_per_second   =      3.924\n",
      "***** test metrics *****\n",
      "  test_f1                 =     0.9986\n",
      "  test_loss               =     0.0046\n",
      "  test_runtime            = 0:32:42.70\n",
      "  test_samples_per_second =     29.005\n",
      "  test_steps_per_second   =      3.626\n",
      "***** train metrics *****\n",
      "  test_f1                 =     0.9986\n",
      "  test_loss               =     0.0046\n",
      "  test_runtime            = 0:32:42.70\n",
      "  test_samples_per_second =     29.005\n",
      "  test_steps_per_second   =      3.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 19:54:53,464 - INFO - Original Test Data Path: /home/ec2-user/SageMaker/dagpap24-submission/notebooks/data/train_data.parquet\n",
      "2024-04-29 19:54:53,465 - INFO - Test Set Predictions path:/home/ec2-user/SageMaker/dagpap24-submission/notebooks/data/output_dev_cs_roberta/test_predictions_cs_roberta.json\n",
      "2024-04-29 19:54:53,466 - INFO - Final Output Path:/home/ec2-user/SageMaker/dagpap24-submission/notebooks/data/predictions_cs_roberta.parquet\n",
      "2024-04-29 19:54:57,511 - INFO - Original Test Data Loaded, (5000, 4)\n",
      "2024-04-29 19:55:00,787 - INFO - Original Test DF = Index(['text', 'annotations', 'tokens', 'token_label_ids'], dtype='object'),                   Index Range = 24993, 10,                  Original Test DF Shape = (5000, 4)\n",
      "2024-04-29 19:55:00,788 - INFO - Predicted DF before apply = Index(['predictions'], dtype='object')\n",
      "2024-04-29 19:55:01,609 - INFO - Predicted DF after apply Info\n",
      "2024-04-29 19:55:01,610 - INFO - Predictions after DF = Index(['predictions', 'preds'], dtype='object'),                   Index Range = 24993, 10,                  Original Test DF Shape = (5000, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final dataset saved to /home/ec2-user/SageMaker/dagpap24-submission/notebooks/data/predictions_cs_roberta.parquet\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "cs_robertamodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9fbc7e-642d-4336-a726-dd363708e204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil \n",
    "import os.path\n",
    "archived = shutil.make_archive('data/cs_roberta_trained', 'zip', 'data/output_dev_cs_roberta')\n",
    "if os.path.exists('data/cs_roberta_trained.zip'):\n",
    "    print(archived) \n",
    "else: \n",
    "    print(\"ZIP file not created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a3308-dfca-4920-a63a-a58714ee0e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d897f4fb-3ad2-46ac-9c92-ace075f1ea57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f096731b-2840-4f3e-b296-764975396a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_preds_df = pd.read_parquet('data/predictions_cs_roberta.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14182d3f-9659-4014-8ead-bc12dd648480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   preds\n",
       "index                                                   \n",
       "10     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
       "13     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "28     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
       "40     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds_df.head()\n",
    "#Add Text, tokens, actual labels, calculate f1 score for the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395ba199-f932-4360-b0d8-24e7fa6016e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_preds_df.rename(columns={'preds': 'cs_roberta_preds'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48993231-838b-4c5f-a579-1409a36311ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_label_ids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15096</th>\n",
       "      <td>Across the world, Emergency Departments are fa...</td>\n",
       "      <td>[[0, 3779, human], [3780, 7601, NLTK_synonym_r...</td>\n",
       "      <td>[Across, the, world,, Emergency, Departments, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428</th>\n",
       "      <td>lung Crab is the in the lead make of cancer-re...</td>\n",
       "      <td>[[0, 4166, NLTK_synonym_replacement], [4167, 2...</td>\n",
       "      <td>[lung, Crab, is, the, in, the, lead, make, of,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>The number of osteoporotic fractures, particul...</td>\n",
       "      <td>[[0, 3264, chatgpt], [3265, 17179, human], [17...</td>\n",
       "      <td>[The, number, of, osteoporotic, fractures,, pa...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5826</th>\n",
       "      <td>The COVID-19 pandemic has spread to every coun...</td>\n",
       "      <td>[[0, 3666, human], [3667, 6954, chatgpt], [695...</td>\n",
       "      <td>[The, COVID-19, pandemic, has, spread, to, eve...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>Endophytic fungi live a significant part of th...</td>\n",
       "      <td>[[0, 10489, human], [10490, 12000, summarized]...</td>\n",
       "      <td>[Endophytic, fungi, live, a, significant, part...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "index                                                      \n",
       "15096  Across the world, Emergency Departments are fa...   \n",
       "14428  lung Crab is the in the lead make of cancer-re...   \n",
       "2144   The number of osteoporotic fractures, particul...   \n",
       "5826   The COVID-19 pandemic has spread to every coun...   \n",
       "1452   Endophytic fungi live a significant part of th...   \n",
       "\n",
       "                                             annotations  \\\n",
       "index                                                      \n",
       "15096  [[0, 3779, human], [3780, 7601, NLTK_synonym_r...   \n",
       "14428  [[0, 4166, NLTK_synonym_replacement], [4167, 2...   \n",
       "2144   [[0, 3264, chatgpt], [3265, 17179, human], [17...   \n",
       "5826   [[0, 3666, human], [3667, 6954, chatgpt], [695...   \n",
       "1452   [[0, 10489, human], [10490, 12000, summarized]...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "15096  [Across, the, world,, Emergency, Departments, ...   \n",
       "14428  [lung, Crab, is, the, in, the, lead, make, of,...   \n",
       "2144   [The, number, of, osteoporotic, fractures,, pa...   \n",
       "5826   [The, COVID-19, pandemic, has, spread, to, eve...   \n",
       "1452   [Endophytic, fungi, live, a, significant, part...   \n",
       "\n",
       "                                         token_label_ids  \n",
       "index                                                     \n",
       "15096  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "14428  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2144   [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "5826   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1452   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_parquet('data/train_data.parquet', engine='fastparquet')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590ec51-b443-47b7-b364-d3c95a0787ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c7142ed-dc1c-42ec-bdee-732e168af995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cs_roberta_preds</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_label_ids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>the world summit on sustainable development fi...</td>\n",
       "      <td>[the, world, summit, on, sustainable, developm...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Bioresorbable implants have progressively move...</td>\n",
       "      <td>[Bioresorbable, implants, have, progressively,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>For centuries, midwives have encouraged women ...</td>\n",
       "      <td>[For, centuries,, midwives, have, encouraged, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>a study of 170 contacts in the uk reported a 5...</td>\n",
       "      <td>[a, study, of, 170, contacts, in, the, uk, rep...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>There is a growing concern over potentially el...</td>\n",
       "      <td>[There, is, a, growing, concern, over, potenti...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cs_roberta_preds  \\\n",
       "index                                                      \n",
       "10     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "13     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "40     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                    text  \\\n",
       "index                                                      \n",
       "10     the world summit on sustainable development fi...   \n",
       "13     Bioresorbable implants have progressively move...   \n",
       "19     For centuries, midwives have encouraged women ...   \n",
       "28     a study of 170 contacts in the uk reported a 5...   \n",
       "40     There is a growing concern over potentially el...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "10     [the, world, summit, on, sustainable, developm...   \n",
       "13     [Bioresorbable, implants, have, progressively,...   \n",
       "19     [For, centuries,, midwives, have, encouraged, ...   \n",
       "28     [a, study, of, 170, contacts, in, the, uk, rep...   \n",
       "40     [There, is, a, growing, concern, over, potenti...   \n",
       "\n",
       "                                         token_label_ids  \n",
       "index                                                     \n",
       "10     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "13     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "28     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "40     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_preds = train_preds_df.merge(train_df[['text', 'tokens', 'token_label_ids']],\n",
    "                                          left_index=True, right_index=True, how='inner')\n",
    "print(merged_train_preds.shape)\n",
    "merged_train_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8b85e4-fa49-456b-a57c-a1925750c79a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "merged_train_preds[\"cs_roberta_f1_score\"] = merged_train_preds.apply(\n",
    "        lambda x: f1_score(\n",
    "            x[\"token_label_ids\"], x[\"cs_roberta_preds\"], average=\"macro\"\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "print(merged_train_preds.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e1bad0a-6b35-43f3-8b54-d281d6b82751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score for cs_roberta train data is: 89.0239%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average F1 score for cs_roberta train data is: {merged_train_preds['cs_roberta_f1_score'].mean()*100:0.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec6c2816-1864-4a3e-b2b3-901a76ab8ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cs_roberta_preds</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_label_ids</th>\n",
       "      <th>cs_roberta_f1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>the world summit on sustainable development fi...</td>\n",
       "      <td>[the, world, summit, on, sustainable, developm...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>0.888261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Bioresorbable implants have progressively move...</td>\n",
       "      <td>[Bioresorbable, implants, have, progressively,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.829883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>For centuries, midwives have encouraged women ...</td>\n",
       "      <td>[For, centuries,, midwives, have, encouraged, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.904718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>a study of 170 contacts in the uk reported a 5...</td>\n",
       "      <td>[a, study, of, 170, contacts, in, the, uk, rep...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>0.858801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>There is a growing concern over potentially el...</td>\n",
       "      <td>[There, is, a, growing, concern, over, potenti...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cs_roberta_preds  \\\n",
       "index                                                      \n",
       "10     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "13     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "40     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                    text  \\\n",
       "index                                                      \n",
       "10     the world summit on sustainable development fi...   \n",
       "13     Bioresorbable implants have progressively move...   \n",
       "19     For centuries, midwives have encouraged women ...   \n",
       "28     a study of 170 contacts in the uk reported a 5...   \n",
       "40     There is a growing concern over potentially el...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "10     [the, world, summit, on, sustainable, developm...   \n",
       "13     [Bioresorbable, implants, have, progressively,...   \n",
       "19     [For, centuries,, midwives, have, encouraged, ...   \n",
       "28     [a, study, of, 170, contacts, in, the, uk, rep...   \n",
       "40     [There, is, a, growing, concern, over, potenti...   \n",
       "\n",
       "                                         token_label_ids  cs_roberta_f1_score  \n",
       "index                                                                          \n",
       "10     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...             0.888261  \n",
       "13     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             0.829883  \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             0.904718  \n",
       "28     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...             0.858801  \n",
       "40     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b68aea9b-96ab-4446-90f4-db8bc94bbc2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_train_preds.rename(columns={'token_label_ids': 'true_labels'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1009ed58-cdce-4fdc-ace7-9fb31a284ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5000.000000\n",
       "mean        0.890239\n",
       "std         0.071771\n",
       "min         0.490759\n",
       "25%         0.859250\n",
       "50%         0.899627\n",
       "75%         0.933423\n",
       "max         1.000000\n",
       "Name: cs_roberta_f1_score, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_preds['cs_roberta_f1_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "193734d4-acce-4b8d-9a60-16922f5eefc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_preds[merged_train_preds['cs_roberta_f1_score'] == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a2421b5-7756-423c-bbdf-a2ef1fbc70c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.91\n",
       "Name: cs_roberta_f1_score, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_preds['cs_roberta_f1_score'].round(2).mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434cc82-6d77-48bf-a915-1797cbe69f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d07c9-7ff5-4531-8308-1e461503e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a08614-c020-403c-9f58-3058670efad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a7a06-5822-457d-afa2-79a31d60e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('data/train_data.parquet', engine='fastparquet')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1bd4f-2e35-4bed-8970-d3854e033b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
