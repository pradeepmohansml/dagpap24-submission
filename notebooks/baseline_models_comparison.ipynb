{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b54d292",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model comparison script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b7bf24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import fastparquet\n",
    "import random\n",
    "import statistics\n",
    "import traceback\n",
    "import ast\n",
    "from collections import Counter\n",
    "import langchain\n",
    "import json\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict, List\n",
    "import re\n",
    "import string\n",
    "from openai import RateLimitError, BadRequestError, APITimeoutError\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "from tqdm.gui import tqdm as tqdm_gui\n",
    "from unittest.mock import patch\n",
    "load_dotenv(\"../_envvars.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f4ec7dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['LLM_MAX_LENGTH']='15000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5c2b41c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request = httpx.Request(\"GET\", \"/\")\n",
    "response = httpx.Response(200, request=request)\n",
    "error_context_length = BadRequestError(\"context length exceeded\", response=response, body=\"\")\n",
    "error_rate_limit = RateLimitError(\"rate limit\",response=response,body=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "43ab44eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fb92c499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contesting_models = ['roberta', 'scibert', 'deberta', 'biomed_roberta', 'cs_roberta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3f7cd446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Make changes in this cell after cs_roberta\n",
    "# Save the predictions_cs_roberta.parquet file to S3 dagpapsubmission\n",
    "\n",
    "total_score = (0.220629 + 0.22031 + 0.220398 + 0.2204659 + 0.2204 )\n",
    "model_scores = {'roberta': 0.22031, 'scibert': 0.220629, 'deberta': 0.220398, 'biomed_roberta': 0.2204659,'cs_roberta':  0.220459 }\n",
    "total_score = sum(list(map(lambda x:model_scores[x],list(model_scores.keys()))))\n",
    "model_weights = {\"scibert\": model_scores['scibert']/total_score,\n",
    "                 \"roberta\": model_scores['roberta']/total_score,\n",
    "                 \"deberta\": model_scores['deberta']/total_score,\n",
    "                 \"biomed_roberta\": model_scores['biomed_roberta']/total_score,\n",
    "                 \"cs_roberta\": model_scores['biomed_roberta']/total_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45c134a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_weights = {'deberta': 0.6644,\n",
    " 'roberta': 0.128,\n",
    " 'scibert': 0.1248,\n",
    " 'biomed_roberta': 0.0614,\n",
    " 'cs_roberta': 0.0214}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96089ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deberta': 0.6644,\n",
       " 'roberta': 0.128,\n",
       " 'scibert': 0.1248,\n",
       " 'biomed_roberta': 0.0614,\n",
       " 'cs_roberta': 0.0214}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a93261",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(model_weights.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7eda88c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_dict = {\"human\":0,\"NLTK_synonym_replacement\":1,\"chatgpt\":2,\"summarized\":3}\n",
    "# class_labels_list = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5306eaca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1e5fe67e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_model_predictions():\n",
    "    dev_df = pq.ParquetDataset('s3://dagpapsubmission/data/data_dev_data.parquet', filesystem=s3).read_pandas().to_pandas()\n",
    "    dev_df[\"tokens\"] = dev_df.tokens.map(lambda x:ast.literal_eval(x.decode()))\n",
    "    \n",
    "    for model in contesting_models:\n",
    "        model_df = pq.ParquetDataset(f's3://dagpapsubmission/predictions_{model}.parquet', filesystem=s3).read_pandas().to_pandas()\n",
    "        model_df.rename(columns={'preds': f'{model}_preds'}, inplace=True)\n",
    "        \n",
    "        dev_df = dev_df.merge(model_df, how='inner', left_index=True, right_index=True)\n",
    "        print(f\"Data shape after merging with {model} model {dev_df.shape}\")\n",
    "    \n",
    "    return dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "39c2dcf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load MAG-FOS Taxonomy JSON for different fields of study\"\n",
    "with open('MAG_FOS.json',\"r+\") as f:\n",
    "    mag_fos_taxonomy = json.load(f)\n",
    "mag_fos_taxonomy\n",
    "major_fields_of_study = list(map(lambda x:x['field_of_study'],mag_fos_taxonomy[\"FOS\"]))\n",
    "major_fields_of_study_str = \",\".join(major_fields_of_study)\n",
    "sub_areas_within_major_field_of_study_list = list(map(lambda x:{x['field_of_study']:x['sub_fields']},mag_fos_taxonomy[\"FOS\"]))\n",
    "sub_areas_within_major_field_of_study = {list(fos.keys())[0]:fos[list(fos.keys())[0]] for fos in sub_areas_within_major_field_of_study_list}\n",
    "sub_areas_within_major_field_of_study_str = \"\\n\".join(f\"{k}:{v}\" for k,v in sub_areas_within_major_field_of_study.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1d0a88ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define Pydantic class for Structured output for article field of study\n",
    "class ArticleFieldOfStudy(BaseModel):\n",
    "    major_field_of_study: str = Field(description=\"The major field of study associated with the text of the article\")\n",
    "    sub_areas_within_major_field_of_study: List[str] = Field(description=\"A list sub areas within the major field of study associated with the text of the article\")\n",
    "    allied_field_of_study: List[str] = Field(description=\"List of other major fields of study associated with the text of the article\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "98a3084b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "article_fos_dict_schema = convert_to_openai_tool(ArticleFieldOfStudy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6fa6326f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n"
     ]
    }
   ],
   "source": [
    "#Setup and test the LLM Instance for all tasks with respect to this analysis\n",
    "llm_models = ['gpt-4-turbo-2024-04-09', 'gpt-3.5-turbo-0125']\n",
    "llms = list(map(lambda x: ChatOpenAI(model=x, temperature=0, max_retries=0,request_timeout=120),llm_models))\n",
    "llm_tests = list(map(lambda x:x.invoke(\"who are you, give me your model name and version?\"),llms))\n",
    "print(llms[1].request_timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "54f6f483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a ChatPromptTempate for executing \n",
    "system = f'''Given an input text from a scientific article identify relevant information about the text.\n",
    "            You can make use of the following major fields of study: {major_fields_of_study_str}\n",
    "            You can also make use of the following sub areas within each major field of study listed above: {sub_areas_within_major_field_of_study_str}\n",
    "         '''\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system), (\"human\", \"{input}\"),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1612ac76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"Given an input text from a scientific article identify relevant information about the text.\\n            You can make use of the following major fields of study: Art,Biology,Business,Chemistry,Computer Science,Economics,Engineering,Environmental Science,Geography,Geology,History,Materials Science,Mathematics,Medicine,Philosophy,Physics,Political Science,Psychology,Sociology\\n            You can also make use of the following sub areas within each major field of study listed above: Art:['Aesthetics, Art History, Classics, Humanities, Literature, Visual Arts']\\nBiology:['Anatomy, Animal Science, Bioinformatics, Botany, Genetics, Immunology, Zoology']\\nBusiness:['Accounting, Actuarial Science, Commerce, Finance, International Trade, Marketing']\\nChemistry:['Biochemistry, Food Science, Mineralogy, Organic Chemistry, Radiochemistry']\\nComputer Science:['Algorithm, Artificial Intelligence, Database, Internet Privacy, Parallel Computing']\\nEconomics:['Accounting, International Trade, Management, Political Economy, Socioeconomics']\\nEngineering:['Aeronautics, Control Theory, Nuclear Engineering, Simulation, Systems-Engineering']\\nEnvironmental Science:['Agricultural Science, Agroforestry, Environmental Planning, Environmental Protection']\\nGeography:['Archaeology, Cartography, Forestry, Geodesy, Meteorology, Regional Science']\\nGeology:['Climatology, Earth Science, Geophysics, Hydrology, Oceanography, Petrology']\\nHistory:['Ancient History, Archaeology, Classics, Economic History, Ethnology, Genealogy']\\nMaterials Science:['Ceramic Materials, Composite Material, Metallurgy, Nanotechnology, Optoelectronics']\\nMathematics:['Algebra, Combinatorics, Geometry, Mathematical Optimization, Statistics, Topology']\\nMedicine:['Audiology, Cancer Research, Nursing, Orthodontics, Pediatrics, Surgery, Virology']\\nPhilosophy:['Aesthetics, Epistemology, Humanities, Linguistics, Religious Studies, Theology']\\nPhysics:['Astronomy, Geophysics, Nuclear Physics, Quantum Mechanics, Thermodynamics']\\nPolitical Science:['Law, Public Administration, Public Relations']\\nPsychology:['Cognitive Science, Criminology, Neuroscience, Psychiatry, Social Psychology']\\nSociology:['Anthropology, Demography, Ethnology, Gender Studies, Media Studies, Political Economy']\\n         \")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f1ae30ad630>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f1ae30acbb0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', request_timeout=120.0, max_retries=0), kwargs={'tools': [{'type': 'function', 'function': {'name': 'ArticleFieldOfStudy', 'description': '', 'parameters': {'type': 'object', 'properties': {'major_field_of_study': {'description': 'The major field of study associated with the text of the article', 'type': 'string'}, 'sub_areas_within_major_field_of_study': {'description': 'A list sub areas within the major field of study associated with the text of the article', 'type': 'array', 'items': {'type': 'string'}}, 'allied_field_of_study': {'description': 'List of other major fields of study associated with the text of the article', 'type': 'array', 'items': {'type': 'string'}}}, 'required': ['major_field_of_study', 'sub_areas_within_major_field_of_study', 'allied_field_of_study']}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'ArticleFieldOfStudy'}}})\n",
       "| JsonOutputKeyToolsParser(first_tool_only=True, key_name='ArticleFieldOfStudy')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#construct structured LLMs from input LLMs\n",
    "structured_llm = llms[1].with_structured_output(article_fos_dict_schema)\n",
    "structured_article_fos_chain = prompt | structured_llm\n",
    "structured_article_fos_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7effb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_repeated_pred(input_df):\n",
    "    input_df['preds'] = None\n",
    "    input_df['majority_model'] = 'baseline'\n",
    "    input_df['majority_col'] = None\n",
    "    for index, row in tqdm(input_df.iterrows(), total=len(input_df)):\n",
    "        combined_preds_max = [0] * len(row['tokens'])\n",
    "        majority_model_prediction = ['baseline'] * len(row['tokens'])\n",
    "        majority_col_val = [0] * len(row['tokens'])\n",
    "    \n",
    "        for i in range(len(row['tokens'])):\n",
    "            preds_data = [row['roberta_preds'][i], row['scibert_preds'][i], row['deberta_preds'][i],\n",
    "                              row['biomed_roberta_preds'][i], row['cs_roberta_preds'][i]]\n",
    "            \n",
    "            max_repeated = statistics.multimode(preds_data)\n",
    "            if len(max_repeated) != 1:\n",
    "                # Weighted avg\n",
    "                combined_preds_max[i] = random.choices(\n",
    "                    preds_data, weights=[model_weights['roberta'], model_weights['scibert'],\n",
    "                                         model_weights['deberta'], model_weights['biomed_roberta'], \n",
    "                                         model_weights['cs_roberta']],\n",
    "                    k=1)[0]\n",
    "                if combined_preds_max[i] == row['deberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'deberta'\n",
    "                elif combined_preds_max[i] == row['biomed_roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'biomed_roberta'\n",
    "                elif combined_preds_max[i] == row['roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'roberta'\n",
    "                elif combined_preds_max[i] == row['cs_roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'cs_roberta'\n",
    "                else:\n",
    "                    majority_model_prediction[i] = 'scibert'\n",
    "                majority_col_val[i] = 0\n",
    "            else:\n",
    "                combined_preds_max[i] = max_repeated[0]\n",
    "                if (row['deberta_preds'][i] == row['biomed_roberta_preds'][i]) and \\\n",
    "                (row['roberta_preds'][i] == row['biomed_roberta_preds'][i]) and \\\n",
    "                (row['roberta_preds'][i] == row['scibert_preds'][i]) and \\\n",
    "                (row['deberta_preds'][i] == row['scibert_preds'][i]) and \\\n",
    "                (row['cs_roberta_preds'][i] == row['scibert_preds'][i]):\n",
    "                    majority_model_prediction[i] = 'all'\n",
    "                else:\n",
    "                    majority_model_prediction[i] = 'majority'\n",
    "                majority_col_val[i] = 1\n",
    "        input_df.at[index,'preds'] = combined_preds_max\n",
    "        input_df.at[index, 'majority_model'] = majority_model_prediction\n",
    "        input_df.at[index, 'majority_col'] = majority_col_val\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7aeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a8bf131",
   "metadata": {},
   "source": [
    "### Find the majority on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ef90da8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 7)\n",
      "Index(['roberta_preds', 'scibert_preds', 'deberta_preds',\n",
      "       'biomed_roberta_preds', 'cs_roberta_preds', 'text', 'tokens'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roberta_preds</th>\n",
       "      <th>scibert_preds</th>\n",
       "      <th>deberta_preds</th>\n",
       "      <th>biomed_roberta_preds</th>\n",
       "      <th>cs_roberta_preds</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>apace increase urbanisation is see as a Major ...</td>\n",
       "      <td>[apace, increase, urbanisation, is, see, as, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Bicycling is a popular leisure activity and an...</td>\n",
       "      <td>[Bicycling, is, a, popular, leisure, activity,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Additive manufacturing (AM) processes are the ...</td>\n",
       "      <td>[Additive, manufacturing, (AM), processes, are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Research in context Unlabelled box Evidence be...</td>\n",
       "      <td>[Research, in, context, Unlabelled, box, Evide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>Mining is an industry that requires a lot of e...</td>\n",
       "      <td>[Mining, is, an, industry, that, requires, a, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                           scibert_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                           deberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                    biomed_roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                        cs_roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                                    text  \\\n",
       "index                                                      \n",
       "0      apace increase urbanisation is see as a Major ...   \n",
       "1      Bicycling is a popular leisure activity and an...   \n",
       "2      Additive manufacturing (AM) processes are the ...   \n",
       "3      Research in context Unlabelled box Evidence be...   \n",
       "4      Mining is an industry that requires a lot of e...   \n",
       "\n",
       "                                                  tokens  \n",
       "index                                                     \n",
       "0      [apace, increase, urbanisation, is, see, as, a...  \n",
       "1      [Bicycling, is, a, popular, leisure, activity,...  \n",
       "2      [Additive, manufacturing, (AM), processes, are...  \n",
       "3      [Research, in, context, Unlabelled, box, Evide...  \n",
       "4      [Mining, is, an, industry, that, requires, a, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_parquet('data/merged_test_predictions_all_models.parquet', engine='fastparquet')\n",
    "print(test_df.shape)\n",
    "print(test_df.columns)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ea9564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 10)\n",
      "Index(['roberta_preds', 'scibert_preds', 'deberta_preds',\n",
      "       'biomed_roberta_preds', 'cs_roberta_preds', 'text', 'tokens', 'preds',\n",
      "       'majority_model', 'majority_col'],\n",
      "      dtype='object')\n",
      "CPU times: user 1h 1min 21s, sys: 1.25 s, total: 1h 1min 22s\n",
      "Wall time: 1h 1min 22s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roberta_preds</th>\n",
       "      <th>scibert_preds</th>\n",
       "      <th>deberta_preds</th>\n",
       "      <th>biomed_roberta_preds</th>\n",
       "      <th>cs_roberta_preds</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>preds</th>\n",
       "      <th>majority_model</th>\n",
       "      <th>majority_col</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>apace increase urbanisation is see as a Major ...</td>\n",
       "      <td>[apace, increase, urbanisation, is, see, as, a...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[all, all, all, all, all, all, all, all, all, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Bicycling is a popular leisure activity and an...</td>\n",
       "      <td>[Bicycling, is, a, popular, leisure, activity,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[all, all, all, all, all, all, all, all, all, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Additive manufacturing (AM) processes are the ...</td>\n",
       "      <td>[Additive, manufacturing, (AM), processes, are...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[all, all, all, all, all, all, all, all, all, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Research in context Unlabelled box Evidence be...</td>\n",
       "      <td>[Research, in, context, Unlabelled, box, Evide...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[all, all, all, all, all, all, all, all, all, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>Mining is an industry that requires a lot of e...</td>\n",
       "      <td>[Mining, is, an, industry, that, requires, a, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[majority, all, all, all, all, all, all, all, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                           scibert_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                           deberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                    biomed_roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                        cs_roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                                    text  \\\n",
       "index                                                      \n",
       "0      apace increase urbanisation is see as a Major ...   \n",
       "1      Bicycling is a popular leisure activity and an...   \n",
       "2      Additive manufacturing (AM) processes are the ...   \n",
       "3      Research in context Unlabelled box Evidence be...   \n",
       "4      Mining is an industry that requires a lot of e...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "0      [apace, increase, urbanisation, is, see, as, a...   \n",
       "1      [Bicycling, is, a, popular, leisure, activity,...   \n",
       "2      [Additive, manufacturing, (AM), processes, are...   \n",
       "3      [Research, in, context, Unlabelled, box, Evide...   \n",
       "4      [Mining, is, an, industry, that, requires, a, ...   \n",
       "\n",
       "                                                   preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                          majority_model  \\\n",
       "index                                                      \n",
       "0      [all, all, all, all, all, all, all, all, all, ...   \n",
       "1      [all, all, all, all, all, all, all, all, all, ...   \n",
       "2      [all, all, all, all, all, all, all, all, all, ...   \n",
       "3      [all, all, all, all, all, all, all, all, all, ...   \n",
       "4      [majority, all, all, all, all, all, all, all, ...   \n",
       "\n",
       "                                            majority_col  \n",
       "index                                                     \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_df = get_max_repeated_pred(test_df)\n",
    "print(test_df.shape)\n",
    "print(test_df.columns)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "524be514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df[['preds']].to_parquet('data/test_predictions_majority_vote.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e99b0b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.to_parquet('data/majority_model_test_predictions_all_cols.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac1f3db0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows not in agreement by all the models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows not in agreement by all the models\")\n",
    "test_df[test_df['majority_model'] != 'all'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d47fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77580a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def parallelize_get_majority_votes(df, )\n",
    "    pool = Pool(30)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56305fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_repeated_pred_row(input_df):\n",
    "    input_df['preds'] = None\n",
    "    input_df['majority_model'] = 'baseline'\n",
    "    input_df['majority_col'] = None\n",
    "    for index, row in tqdm(input_df.iterrows(), total=len(input_df)):\n",
    "        combined_preds_max = [0] * len(row['tokens'])\n",
    "        majority_model_prediction = ['baseline'] * len(row['tokens'])\n",
    "        majority_col_val = [0] * len(row['tokens'])\n",
    "    \n",
    "        for i in range(len(row['tokens'])):\n",
    "            preds_data = [row['roberta_preds'][i], row['scibert_preds'][i], row['deberta_preds'][i],\n",
    "                              row['biomed_roberta_preds'][i], row['cs_roberta_preds'][i]]\n",
    "            \n",
    "            max_repeated = statistics.multimode(preds_data)\n",
    "            if len(max_repeated) != 1:\n",
    "                # Weighted avg\n",
    "                combined_preds_max[i] = random.choices(\n",
    "                    preds_data, weights=[model_weights['roberta'], model_weights['scibert'],\n",
    "                                         model_weights['deberta'], model_weights['biomed_roberta'], \n",
    "                                         model_weights['cs_roberta']],\n",
    "                    k=1)[0]\n",
    "                if combined_preds_max[i] == row['deberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'deberta'\n",
    "                elif combined_preds_max[i] == row['biomed_roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'biomed_roberta'\n",
    "                elif combined_preds_max[i] == row['roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'roberta'\n",
    "                elif combined_preds_max[i] == row['cs_roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'cs_roberta'\n",
    "                else:\n",
    "                    majority_model_prediction[i] = 'scibert'\n",
    "                majority_col_val[i] = 0\n",
    "            else:\n",
    "                combined_preds_max[i] = max_repeated[0]\n",
    "                if (row['deberta_preds'][i] == row['biomed_roberta_preds'][i]) and \\\n",
    "                (row['roberta_preds'][i] == row['biomed_roberta_preds'][i]) and \\\n",
    "                (row['roberta_preds'][i] == row['scibert_preds'][i]) and \\\n",
    "                (row['deberta_preds'][i] == row['scibert_preds'][i]) and \\\n",
    "                (row['cs_roberta_preds'][i] == row['scibert_preds'][i]):\n",
    "                    majority_model_prediction[i] = 'all'\n",
    "                else:\n",
    "                    majority_model_prediction[i] = 'majority'\n",
    "                majority_col_val[i] = 1\n",
    "        input_df.at[index,'preds'] = combined_preds_max\n",
    "        input_df.at[index, 'majority_model'] = majority_model_prediction\n",
    "        input_df.at[index, 'majority_col'] = majority_col_val\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1745a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8481db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44f016ff",
   "metadata": {},
   "source": [
    "### Function calls to make the predictions and save the parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca87fe65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after merging with roberta model (5000, 3)\n",
      "Data shape after merging with scibert model (5000, 4)\n",
      "Data shape after merging with deberta model (5000, 5)\n",
      "Data shape after merging with biomed_roberta model (5000, 6)\n",
      "Data shape after merging with cs_roberta model (5000, 7)\n",
      "CPU times: user 1min 30s, sys: 2.37 s, total: 1min 32s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merged_model_dev_predictions = merge_model_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a47c97ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>roberta_preds</th>\n",
       "      <th>scibert_preds</th>\n",
       "      <th>deberta_preds</th>\n",
       "      <th>biomed_roberta_preds</th>\n",
       "      <th>cs_roberta_preds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12313</th>\n",
       "      <td>Phylogenetic networks are a generalization of ...</td>\n",
       "      <td>[Phylogenetic, networks, are, a, generalizatio...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>Prediction modelling is more closely aligned w...</td>\n",
       "      <td>[Prediction, modelling, is, more, closely, ali...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451</th>\n",
       "      <td>The heat transfer exhibits the flow of heat (t...</td>\n",
       "      <td>[The, heat, transfer, exhibits, the, flow, of,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4351</th>\n",
       "      <td>a common experience during superficial ultraso...</td>\n",
       "      <td>[a, common, experience, during, superficial, u...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22694</th>\n",
       "      <td>Code metadata Current code version v1.5.9 Perm...</td>\n",
       "      <td>[Code, metadata, Current, code, version, v1.5....</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "index                                                      \n",
       "12313  Phylogenetic networks are a generalization of ...   \n",
       "3172   Prediction modelling is more closely aligned w...   \n",
       "6451   The heat transfer exhibits the flow of heat (t...   \n",
       "4351   a common experience during superficial ultraso...   \n",
       "22694  Code metadata Current code version v1.5.9 Perm...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "12313  [Phylogenetic, networks, are, a, generalizatio...   \n",
       "3172   [Prediction, modelling, is, more, closely, ali...   \n",
       "6451   [The, heat, transfer, exhibits, the, flow, of,...   \n",
       "4351   [a, common, experience, during, superficial, u...   \n",
       "22694  [Code, metadata, Current, code, version, v1.5....   \n",
       "\n",
       "                                           roberta_preds  \\\n",
       "index                                                      \n",
       "12313  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3172   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6451   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4351   [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "22694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           scibert_preds  \\\n",
       "index                                                      \n",
       "12313  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3172   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6451   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4351   [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, ...   \n",
       "22694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           deberta_preds  \\\n",
       "index                                                      \n",
       "12313  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3172   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6451   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4351   [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "22694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    biomed_roberta_preds  \\\n",
       "index                                                      \n",
       "12313  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3172   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6451   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4351   [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "22694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        cs_roberta_preds  \n",
       "index                                                     \n",
       "12313  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3172   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6451   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4351   [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "22694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_dev_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d2b8ef33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 s, sys: 0 ns, total: 4 s\n",
      "Wall time: 7.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#outputs = list(map(lambda x:x.invoke({\"input\":input_text}),structured_article_fos_chains))\n",
    "chain = structured_article_fos_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84f95f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text_with_index = pd.DataFrame(merged_model_dev_predictions['text'])\n",
    "input_text_with_index_dict = input_text_with_index.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585f25b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def create_batches(input_text_index_dict):\n",
    "    length_of_data = len(list(input_text_index_dict.keys()))\n",
    "    batch_size = 100\n",
    "    nbatches = math.ceil(length_of_data/batch_size)\n",
    "    batches_of_keys = []\n",
    "    key_list = list(input_text_index_dict.keys())\n",
    "    for i in range(0,nbatches):\n",
    "        a_batch_of_keys = key_list[i*batch_size:(i+1)*batch_size]\n",
    "        batches_of_keys.append(a_batch_of_keys)\n",
    "    batches = list(map(lambda x:{key:input_text_index_dict.get(key,\"\")[\"text\"] for key in x},batches_of_keys))\n",
    "    return batches\n",
    "batches = create_batches(input_text_with_index_dict)\n",
    "batch_sizes = [len(b) for b in batches]\n",
    "batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152d6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = batches[0]\n",
    "input_texts = [batch[key] for key in list(batch.keys())]\n",
    "results = chain.batch(input_texts)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ac7e7be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def populate_field_of_study(batches,load_last_checkpt=None,checkpt=True):\n",
    "    LLM_MAX_LENGTH = os.getenv('LLM_MAX_LENGTH',16000)\n",
    "    LLM_MAX_LENGTH=int(LLM_MAX_LENGTH)\n",
    "    results_batches = []\n",
    "    rate_limit_delay = 5\n",
    "    checkpt_dict={}\n",
    "    i=0\n",
    "    while i < len(batches):\n",
    "        if load_last_checkpt is not None:\n",
    "            with open(load_last_checkpt,\"r+\") as f:\n",
    "                checkpt_dict=json.load(f)\n",
    "            f.close()\n",
    "            i = int(load_last_checkpt.strip('.json').split('_')[2])-1\n",
    "            print(f\"Loaded Last checkpointed batch {i+1}\")\n",
    "            results_batches = checkpt_dict[f\"Batch_{i+1}\"]\n",
    "            last_chekpt = load_last_checkpt.strip('.json')\n",
    "            load_last_checkpt = None\n",
    "            i+=1\n",
    "            if i<len(batches):\n",
    "                batch = batches[i]\n",
    "            else:\n",
    "                return results_batches, last_chekpt\n",
    "        else:\n",
    "            batch = batches[i]\n",
    "        print(f\"Processing Batch {i+1}\")\n",
    "        input_texts=[batch[key] for key in list(batch.keys())]\n",
    "        try:\n",
    "            results = chain.batch(input_texts)\n",
    "        except APITimeoutError:\n",
    "            checkpt_dict[f\"Batch_{i+1}\"] = results_batches\n",
    "            with open(f\"checkpt_batch_{i+1}.json\",\"w+\") as f:\n",
    "                json.dump(checkpt_dict,f)\n",
    "            f.close()\n",
    "            last_checkpt=f\"checkpt_batch_{i+1}\"\n",
    "        except RateLimitError:\n",
    "            delay = 30\n",
    "            print(f\"Rate Limit Error Encountered, sleeping for {delay} seconds\")\n",
    "            time.sleep(delay)\n",
    "            results = chain.batch(input_texts)\n",
    "            rate_limit_delay *=2\n",
    "            print(f\"Doubling rate limit delay between batches to {rate_limit_delay} seconds\")\n",
    "        except BadRequestError:\n",
    "            print(f\"Bad Request Error Hit, adjusting input text length to accomodate model context\")\n",
    "            encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "            encoded_input_texts = list(map(lambda x:encoding.encode(x),input_texts))\n",
    "            num_tokens_for_texts = list(map(lambda x:len(x),encoded_input_texts))\n",
    "            clipped_input_texts = []\n",
    "            for j,encoded_text in enumerate(encoded_input_texts):\n",
    "                num_tokens = len(encoded_text)\n",
    "                if num_tokens > LLM_MAX_LENGTH:\n",
    "                    print(f\"In Batch {i+1}, text {j+1} has {num_tokens} tokens, reducing it down to {LLM_MAX_LENGTH}\")\n",
    "                    clipped_input_texts.append(encoding.decode(encoded_text[:LLM_MAX_LENGTH]))\n",
    "                else:\n",
    "                    #print(f\"In Batch {i+1}, text {j+1} has {num_tokens} tokens, NOT reducing the tokens\")\n",
    "                    clipped_input_texts.append(encoding.decode(encoded_text))\n",
    "            try:\n",
    "                results = chain.batch(clipped_input_texts)\n",
    "            except RateLimitError:\n",
    "                delay = 30\n",
    "                print(f\"Rate Limit Error Encountered, sleeping for {delay} seconds\")\n",
    "                time.sleep(delay)\n",
    "                results = chain.batch(clipped_input_texts)\n",
    "                rate_limit_delay *=2\n",
    "                print(f\"Doubling rate limit delay between batches to {rate_limit_delay} seconds\")\n",
    "        batch_result_dict = {key:results[i] for i,key in enumerate(list(batch.keys()))}\n",
    "        results_batches.append(batch_result_dict)\n",
    "        if checkpt:\n",
    "            checkpt_dict[f\"Batch_{i+1}\"] = results_batches\n",
    "            with open(f\"checkpt_batch_{i+1}.json\",\"w+\") as f:\n",
    "                json.dump(checkpt_dict,f)\n",
    "            f.close()\n",
    "            last_checkpt=f\"checkpt_batch_{i+1}\"\n",
    "        if i+2<=len(batches):\n",
    "            print(f\"Sleeping for {rate_limit_delay} seconds before processing batch {i+2}\")\n",
    "        time.sleep(rate_limit_delay)\n",
    "        if i<len(batches):\n",
    "            i+=1\n",
    "    return results_batches, last_chekpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc81b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "last_checkpt=None\n",
    "results_batches, last_checkpt = populate_field_of_study(batches,load_last_checkpt=f\"{last_checkpt}.json\" if last_checkpt is not None else None)\n",
    "last_checkpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_dev_predictions[['major_field_of_study','sub_areas_within_major_field_of_study','allied_fields_of_study']] = merged_model_dev_predictions.apply(lambda x:populate_field_of_study(x),axis=1,result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ab3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model_dev_predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96532b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "majority_vote_df = get_max_repeated_pred(merged_model_dev_predictions)\n",
    "majority_vote_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e31163",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote_df[['preds']].to_parquet('predictions_four_models.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e374e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "majority_vote_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428890a1",
   "metadata": {},
   "source": [
    "# End here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02184747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca4fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb0003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4aac685",
   "metadata": {},
   "source": [
    "### Finding the stats about the majority column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59169173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_stats(row):\n",
    "    stats_dict = row['stats']\n",
    "    out_dict = {}\n",
    "    out_keys =  ['roberta', 'scibert', 'deberta', 'biomed_roberta',  'all']\n",
    "    out_dict = {key:stats_dict[key] if key in list(stats_dict.keys()) else 0 for key in out_keys}\n",
    "    \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc90ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "majority_vote_df['stats'] = majority_vote_df[['majority_model']].map(lambda x: Counter(x))\n",
    "# majority_vote_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a8847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df = majority_vote_df[['tokens', 'stats', 'majority_col']]\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87beb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['len_tokens'] = stats_df['tokens'].map(lambda x : len(x))\n",
    "stats_df.drop(columns=['tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629cdca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['sum_majority'] = stats_df['majority_col'].map(lambda x : sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacdec70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['sum_majority_not_all'] = stats_df['sum_majority'] - stats_df['all']\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab95b381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['pct_majority'] = 100 * stats_df['sum_majority']/stats_df['len_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806180d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ae713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df[['roberta', 'scibert', 'deberta', 'biomed_roberta',  'all']] = stats_df.apply(\n",
    "    lambda x : get_model_stats(x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6e792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60004a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['pct_majority'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2992dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['roberta'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfca6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['scibert'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d75746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['deberta'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f142b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['biomed_roberta'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5ae6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['all'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6ed8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7be84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07b01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "255ef516",
   "metadata": {},
   "source": [
    "# For local runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"/Users/gayatri/Documents/Gayatri/US/Self projects/AI Competition/DAGPAP24/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a3b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_parquet(base_folder + os.sep + 'dev_data.parquet', engine=\"fastparquet\")\n",
    "print(dev_df.shape)\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a8ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = dev_df.copy(deep=True)\n",
    "\n",
    "for model in contesting_models:\n",
    "    model_df = pd.read_parquet(base_folder + os.sep + f'predictions_{model}.parquet', engine=\"fastparquet\")\n",
    "    model_df.rename(columns={'preds': f'{model}_preds'}, inplace=True)\n",
    "\n",
    "    merged = merged.merge(model_df, how='inner', left_index=True, right_index=True)\n",
    "    print(f\"Data shape after merging with {model} model {merged.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a25cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_pred_df = get_max_repeated_pred(merged)\n",
    "print(max_pred_df.shape)\n",
    "max_pred_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.88/(0.87+0.88+0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c70f860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pred_df[['preds']].to_parquet(base_folder + os.sep + 'predictions_three_models_combined.parquet') # , engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e197ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(base_folder + os.sep + 'dev_majority_model_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbddec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_parquet(base_folder + os.sep + 'dev_majority_model_preds.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb347f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225b96f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
