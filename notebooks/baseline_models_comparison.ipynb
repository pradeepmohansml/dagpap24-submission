{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed80b5b2-e96b-45c9-a319-c62c26a598f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model comparison script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afb94af-8ad3-45ca-8258-a9463b9614a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import fastparquet\n",
    "import random\n",
    "import statistics\n",
    "import traceback\n",
    "import ast\n",
    "from collections import Counter\n",
    "import langchain\n",
    "import json\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict, List\n",
    "import re\n",
    "import string\n",
    "from openai import RateLimitError, BadRequestError, APITimeoutError\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "from tqdm.gui import tqdm as tqdm_gui\n",
    "from unittest.mock import patch\n",
    "load_dotenv(\"../_envvars.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4c20c71c-2670-473e-bd9d-a8f1ea751832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['LLM_MAX_LENGTH']='15000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "99713d65-2819-41fc-a605-7f6fe057075e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request = httpx.Request(\"GET\", \"/\")\n",
    "response = httpx.Response(200, request=request)\n",
    "error_context_length = BadRequestError(\"context length exceeded\", response=response, body=\"\")\n",
    "error_rate_limit = RateLimitError(\"rate limit\",response=response,body=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b04c2717-9adf-4aed-a2ae-2da1115dbcf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0fb07215-c311-4e75-9c33-ee3390bec919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contesting_models = ['roberta', 'scibert', 'deberta', 'biomed_roberta', 'cs_roberta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "55ab3053-7ac3-4dc5-a807-56adcc0a8a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Make changes in this cell after cs_roberta\n",
    "# Save the predictions_cs_roberta.parquet file to S3 dagpapsubmission\n",
    "\n",
    "total_score = (0.220629 + 0.22031 + 0.220398 + 0.2204659 + 0.2204 )\n",
    "model_scores = {'roberta': 0.22031, 'scibert': 0.220629, 'deberta': 0.220398, 'biomed_roberta': 0.2204659,'cs_roberta':  0.220459 }\n",
    "total_score = sum(list(map(lambda x:model_scores[x],list(model_scores.keys()))))\n",
    "model_weights = {\"scibert\": model_scores['scibert']/total_score,\n",
    "                 \"roberta\": model_scores['roberta']/total_score,\n",
    "                 \"deberta\": model_scores['deberta']/total_score,\n",
    "                 \"biomed_roberta\": model_scores['biomed_roberta']/total_score,\n",
    "                 \"cs_roberta\": model_scores['biomed_roberta']/total_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f279c9a1-b0b6-473a-84b2-37a1a8794315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_weights = {'deberta': 0.6644,\n",
    " 'roberta': 0.128,\n",
    " 'scibert': 0.1248,\n",
    " 'biomed_roberta': 0.0614,\n",
    " 'cs_roberta': 0.0214}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23bb6f78-0fd6-46cb-92be-c1161130fbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deberta': 0.6644,\n",
       " 'roberta': 0.128,\n",
       " 'scibert': 0.1248,\n",
       " 'biomed_roberta': 0.0614,\n",
       " 'cs_roberta': 0.0214}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c95e45d-4e4e-41d9-9d41-2440062a3902",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(model_weights.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4bea9938-dcf0-4b0e-a824-05147807b1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_dict = {\"human\":0,\"NLTK_synonym_replacement\":1,\"chatgpt\":2,\"summarized\":3}\n",
    "# class_labels_list = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e052783-fda7-4928-a611-ed0885145141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7d61b236-6e09-46da-9eaf-c154416c1c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_model_predictions():\n",
    "    dev_df = pq.ParquetDataset('s3://dagpapsubmission/data/data_dev_data.parquet', filesystem=s3).read_pandas().to_pandas()\n",
    "    dev_df[\"tokens\"] = dev_df.tokens.map(lambda x:ast.literal_eval(x.decode()))\n",
    "    \n",
    "    for model in contesting_models:\n",
    "        model_df = pq.ParquetDataset(f's3://dagpapsubmission/predictions_{model}.parquet', filesystem=s3).read_pandas().to_pandas()\n",
    "        model_df.rename(columns={'preds': f'{model}_preds'}, inplace=True)\n",
    "        \n",
    "        dev_df = dev_df.merge(model_df, how='inner', left_index=True, right_index=True)\n",
    "        print(f\"Data shape after merging with {model} model {dev_df.shape}\")\n",
    "    \n",
    "    return dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b1527e1f-0e9b-4cdf-abff-e50f670481f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load MAG-FOS Taxonomy JSON for different fields of study\"\n",
    "with open('MAG_FOS.json',\"r+\") as f:\n",
    "    mag_fos_taxonomy = json.load(f)\n",
    "mag_fos_taxonomy\n",
    "major_fields_of_study = list(map(lambda x:x['field_of_study'],mag_fos_taxonomy[\"FOS\"]))\n",
    "major_fields_of_study_str = \",\".join(major_fields_of_study)\n",
    "sub_areas_within_major_field_of_study_list = list(map(lambda x:{x['field_of_study']:x['sub_fields']},mag_fos_taxonomy[\"FOS\"]))\n",
    "sub_areas_within_major_field_of_study = {list(fos.keys())[0]:fos[list(fos.keys())[0]] for fos in sub_areas_within_major_field_of_study_list}\n",
    "sub_areas_within_major_field_of_study_str = \"\\n\".join(f\"{k}:{v}\" for k,v in sub_areas_within_major_field_of_study.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "94ecf6d5-fc8f-4d1f-b0b4-de79bdc9ac6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define Pydantic class for Structured output for article field of study\n",
    "class ArticleFieldOfStudy(BaseModel):\n",
    "    major_field_of_study: str = Field(description=\"The major field of study associated with the text of the article\")\n",
    "    sub_areas_within_major_field_of_study: List[str] = Field(description=\"A list sub areas within the major field of study associated with the text of the article\")\n",
    "    allied_field_of_study: List[str] = Field(description=\"List of other major fields of study associated with the text of the article\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3e5b3851-139b-4337-9348-fd14ee478716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "article_fos_dict_schema = convert_to_openai_tool(ArticleFieldOfStudy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5a49bfbb-a42d-459d-8a9d-e2a09a7f4bda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n"
     ]
    }
   ],
   "source": [
    "#Setup and test the LLM Instance for all tasks with respect to this analysis\n",
    "llm_models = ['gpt-4-turbo-2024-04-09', 'gpt-3.5-turbo-0125']\n",
    "llms = list(map(lambda x: ChatOpenAI(model=x, temperature=0, max_retries=0,request_timeout=120),llm_models))\n",
    "llm_tests = list(map(lambda x:x.invoke(\"who are you, give me your model name and version?\"),llms))\n",
    "print(llms[1].request_timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e3872acb-0b6a-48c9-8251-8090391cb4f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a ChatPromptTempate for executing \n",
    "system = f'''Given an input text from a scientific article identify relevant information about the text.\n",
    "            You can make use of the following major fields of study: {major_fields_of_study_str}\n",
    "            You can also make use of the following sub areas within each major field of study listed above: {sub_areas_within_major_field_of_study_str}\n",
    "         '''\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system), (\"human\", \"{input}\"),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "52dc10be-dbd5-4132-9d02-31a5a7ac3d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"Given an input text from a scientific article identify relevant information about the text.\\n            You can make use of the following major fields of study: Art,Biology,Business,Chemistry,Computer Science,Economics,Engineering,Environmental Science,Geography,Geology,History,Materials Science,Mathematics,Medicine,Philosophy,Physics,Political Science,Psychology,Sociology\\n            You can also make use of the following sub areas within each major field of study listed above: Art:['Aesthetics, Art History, Classics, Humanities, Literature, Visual Arts']\\nBiology:['Anatomy, Animal Science, Bioinformatics, Botany, Genetics, Immunology, Zoology']\\nBusiness:['Accounting, Actuarial Science, Commerce, Finance, International Trade, Marketing']\\nChemistry:['Biochemistry, Food Science, Mineralogy, Organic Chemistry, Radiochemistry']\\nComputer Science:['Algorithm, Artificial Intelligence, Database, Internet Privacy, Parallel Computing']\\nEconomics:['Accounting, International Trade, Management, Political Economy, Socioeconomics']\\nEngineering:['Aeronautics, Control Theory, Nuclear Engineering, Simulation, Systems-Engineering']\\nEnvironmental Science:['Agricultural Science, Agroforestry, Environmental Planning, Environmental Protection']\\nGeography:['Archaeology, Cartography, Forestry, Geodesy, Meteorology, Regional Science']\\nGeology:['Climatology, Earth Science, Geophysics, Hydrology, Oceanography, Petrology']\\nHistory:['Ancient History, Archaeology, Classics, Economic History, Ethnology, Genealogy']\\nMaterials Science:['Ceramic Materials, Composite Material, Metallurgy, Nanotechnology, Optoelectronics']\\nMathematics:['Algebra, Combinatorics, Geometry, Mathematical Optimization, Statistics, Topology']\\nMedicine:['Audiology, Cancer Research, Nursing, Orthodontics, Pediatrics, Surgery, Virology']\\nPhilosophy:['Aesthetics, Epistemology, Humanities, Linguistics, Religious Studies, Theology']\\nPhysics:['Astronomy, Geophysics, Nuclear Physics, Quantum Mechanics, Thermodynamics']\\nPolitical Science:['Law, Public Administration, Public Relations']\\nPsychology:['Cognitive Science, Criminology, Neuroscience, Psychiatry, Social Psychology']\\nSociology:['Anthropology, Demography, Ethnology, Gender Studies, Media Studies, Political Economy']\\n         \")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f1ae30ad630>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f1ae30acbb0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', request_timeout=120.0, max_retries=0), kwargs={'tools': [{'type': 'function', 'function': {'name': 'ArticleFieldOfStudy', 'description': '', 'parameters': {'type': 'object', 'properties': {'major_field_of_study': {'description': 'The major field of study associated with the text of the article', 'type': 'string'}, 'sub_areas_within_major_field_of_study': {'description': 'A list sub areas within the major field of study associated with the text of the article', 'type': 'array', 'items': {'type': 'string'}}, 'allied_field_of_study': {'description': 'List of other major fields of study associated with the text of the article', 'type': 'array', 'items': {'type': 'string'}}}, 'required': ['major_field_of_study', 'sub_areas_within_major_field_of_study', 'allied_field_of_study']}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'ArticleFieldOfStudy'}}})\n",
       "| JsonOutputKeyToolsParser(first_tool_only=True, key_name='ArticleFieldOfStudy')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#construct structured LLMs from input LLMs\n",
    "structured_llm = llms[1].with_structured_output(article_fos_dict_schema)\n",
    "structured_article_fos_chain = prompt | structured_llm\n",
    "structured_article_fos_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eebb681-483f-4e02-a237-f256f5a45d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_repeated_pred(input_df):\n",
    "    input_df['preds'] = None\n",
    "    input_df['majority_model'] = 'baseline'\n",
    "    input_df['majority_col'] = None\n",
    "    for index, row in tqdm(input_df.iterrows(), total=len(input_df)):\n",
    "        combined_preds_max = [0] * len(row['tokens'])\n",
    "        majority_model_prediction = ['baseline'] * len(row['tokens'])\n",
    "        majority_col_val = [0] * len(row['tokens'])\n",
    "    \n",
    "        for i in range(len(row['tokens'])):\n",
    "            preds_data = [row['roberta_preds'][i], row['scibert_preds'][i], row['deberta_preds'][i],\n",
    "                              row['biomed_roberta_preds'][i], row['cs_roberta_preds'][i]]\n",
    "            \n",
    "            max_repeated = statistics.multimode(preds_data)\n",
    "            if len(max_repeated) != 1:\n",
    "                # Weighted avg\n",
    "                combined_preds_max[i] = random.choices(\n",
    "                    preds_data, weights=[model_weights['roberta'], model_weights['scibert'],\n",
    "                                         model_weights['deberta'], model_weights['biomed_roberta'], \n",
    "                                         model_weights['cs_roberta']],\n",
    "                    k=1)[0]\n",
    "                if combined_preds_max[i] == row['deberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'deberta'\n",
    "                elif combined_preds_max[i] == row['biomed_roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'biomed_roberta'\n",
    "                elif combined_preds_max[i] == row['roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'roberta'\n",
    "                elif combined_preds_max[i] == row['cs_roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'cs_roberta'\n",
    "                else:\n",
    "                    majority_model_prediction[i] = 'scibert'\n",
    "                majority_col_val[i] = 0\n",
    "            else:\n",
    "                combined_preds_max[i] = max_repeated[0]\n",
    "                if (row['deberta_preds'][i] == row['biomed_roberta_preds'][i]) and \\\n",
    "                (row['roberta_preds'][i] == row['biomed_roberta_preds'][i]) and \\\n",
    "                (row['roberta_preds'][i] == row['scibert_preds'][i]) and \\\n",
    "                (row['deberta_preds'][i] == row['scibert_preds'][i]) and \\\n",
    "                (row['cs_roberta_preds'][i] == row['scibert_preds'][i]):\n",
    "                    majority_model_prediction[i] = 'all'\n",
    "                else:\n",
    "                    majority_model_prediction[i] = 'majority'\n",
    "                majority_col_val[i] = 1\n",
    "        input_df.at[index,'preds'] = combined_preds_max\n",
    "        input_df.at[index, 'majority_model'] = majority_model_prediction\n",
    "        input_df.at[index, 'majority_col'] = majority_col_val\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d53a99-85b9-4bae-8608-30d17ce7f155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "954f7a9b-ed76-4fbd-ae5d-fce2005bf593",
   "metadata": {},
   "source": [
    "### Find the majority on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51887e27-664d-4ebc-a231-cc8e8038fda4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 7)\n",
      "Index(['roberta_preds', 'scibert_preds', 'deberta_preds',\n",
      "       'biomed_roberta_preds', 'cs_roberta_preds', 'text', 'tokens'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roberta_preds</th>\n",
       "      <th>scibert_preds</th>\n",
       "      <th>deberta_preds</th>\n",
       "      <th>biomed_roberta_preds</th>\n",
       "      <th>cs_roberta_preds</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>apace increase urbanisation is see as a Major ...</td>\n",
       "      <td>[apace, increase, urbanisation, is, see, as, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Bicycling is a popular leisure activity and an...</td>\n",
       "      <td>[Bicycling, is, a, popular, leisure, activity,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Additive manufacturing (AM) processes are the ...</td>\n",
       "      <td>[Additive, manufacturing, (AM), processes, are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Research in context Unlabelled box Evidence be...</td>\n",
       "      <td>[Research, in, context, Unlabelled, box, Evide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>Mining is an industry that requires a lot of e...</td>\n",
       "      <td>[Mining, is, an, industry, that, requires, a, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                           scibert_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                           deberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                    biomed_roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                        cs_roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                                    text  \\\n",
       "index                                                      \n",
       "0      apace increase urbanisation is see as a Major ...   \n",
       "1      Bicycling is a popular leisure activity and an...   \n",
       "2      Additive manufacturing (AM) processes are the ...   \n",
       "3      Research in context Unlabelled box Evidence be...   \n",
       "4      Mining is an industry that requires a lot of e...   \n",
       "\n",
       "                                                  tokens  \n",
       "index                                                     \n",
       "0      [apace, increase, urbanisation, is, see, as, a...  \n",
       "1      [Bicycling, is, a, popular, leisure, activity,...  \n",
       "2      [Additive, manufacturing, (AM), processes, are...  \n",
       "3      [Research, in, context, Unlabelled, box, Evide...  \n",
       "4      [Mining, is, an, industry, that, requires, a, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_parquet('data/merged_test_predictions_all_models.parquet', engine='fastparquet')\n",
    "print(test_df.shape)\n",
    "print(test_df.columns)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37482abc-8901-4ce7-9833-6793953efcf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 10)\n",
      "Index(['roberta_preds', 'scibert_preds', 'deberta_preds',\n",
      "       'biomed_roberta_preds', 'cs_roberta_preds', 'text', 'tokens', 'preds',\n",
      "       'majority_model', 'majority_col'],\n",
      "      dtype='object')\n",
      "CPU times: user 1h 1min 21s, sys: 1.25 s, total: 1h 1min 22s\n",
      "Wall time: 1h 1min 22s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roberta_preds</th>\n",
       "      <th>scibert_preds</th>\n",
       "      <th>deberta_preds</th>\n",
       "      <th>biomed_roberta_preds</th>\n",
       "      <th>cs_roberta_preds</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>preds</th>\n",
       "      <th>majority_model</th>\n",
       "      <th>majority_col</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>apace increase urbanisation is see as a Major ...</td>\n",
       "      <td>[apace, increase, urbanisation, is, see, as, a...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[all, all, all, all, all, all, all, all, all, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Bicycling is a popular leisure activity and an...</td>\n",
       "      <td>[Bicycling, is, a, popular, leisure, activity,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[all, all, all, all, all, all, all, all, all, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Additive manufacturing (AM) processes are the ...</td>\n",
       "      <td>[Additive, manufacturing, (AM), processes, are...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[all, all, all, all, all, all, all, all, all, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Research in context Unlabelled box Evidence be...</td>\n",
       "      <td>[Research, in, context, Unlabelled, box, Evide...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[all, all, all, all, all, all, all, all, all, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>Mining is an industry that requires a lot of e...</td>\n",
       "      <td>[Mining, is, an, industry, that, requires, a, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[majority, all, all, all, all, all, all, all, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                           scibert_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                           deberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                    biomed_roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                        cs_roberta_preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                                    text  \\\n",
       "index                                                      \n",
       "0      apace increase urbanisation is see as a Major ...   \n",
       "1      Bicycling is a popular leisure activity and an...   \n",
       "2      Additive manufacturing (AM) processes are the ...   \n",
       "3      Research in context Unlabelled box Evidence be...   \n",
       "4      Mining is an industry that requires a lot of e...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "0      [apace, increase, urbanisation, is, see, as, a...   \n",
       "1      [Bicycling, is, a, popular, leisure, activity,...   \n",
       "2      [Additive, manufacturing, (AM), processes, are...   \n",
       "3      [Research, in, context, Unlabelled, box, Evide...   \n",
       "4      [Mining, is, an, industry, that, requires, a, ...   \n",
       "\n",
       "                                                   preds  \\\n",
       "index                                                      \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                          majority_model  \\\n",
       "index                                                      \n",
       "0      [all, all, all, all, all, all, all, all, all, ...   \n",
       "1      [all, all, all, all, all, all, all, all, all, ...   \n",
       "2      [all, all, all, all, all, all, all, all, all, ...   \n",
       "3      [all, all, all, all, all, all, all, all, all, ...   \n",
       "4      [majority, all, all, all, all, all, all, all, ...   \n",
       "\n",
       "                                            majority_col  \n",
       "index                                                     \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_df = get_max_repeated_pred(test_df)\n",
    "print(test_df.shape)\n",
    "print(test_df.columns)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc41fffb-9c32-4bb5-a0df-9547f1b061da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df[['preds']].to_parquet('data/test_predictions_majority_vote.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "814ee65d-b7f3-4377-9935-84ef987e38dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.to_parquet('data/majority_model_test_predictions_all_cols.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56f923a5-ab9b-4197-8f71-231b79b9c8a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows not in agreement by all the models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows not in agreement by all the models\")\n",
    "test_df[test_df['majority_model'] != 'all'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f5279-fea2-49fc-937f-ec16e74fb544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b678e85-6709-44e9-bfef-38e4bab5aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def parallelize_get_majority_votes(df, )\n",
    "    pool = Pool(30)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0069a318-f9c8-4e35-9aad-e75fb07bbbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_repeated_pred_row(input_df):\n",
    "    input_df['preds'] = None\n",
    "    input_df['majority_model'] = 'baseline'\n",
    "    input_df['majority_col'] = None\n",
    "    for index, row in tqdm(input_df.iterrows(), total=len(input_df)):\n",
    "        combined_preds_max = [0] * len(row['tokens'])\n",
    "        majority_model_prediction = ['baseline'] * len(row['tokens'])\n",
    "        majority_col_val = [0] * len(row['tokens'])\n",
    "    \n",
    "        for i in range(len(row['tokens'])):\n",
    "            preds_data = [row['roberta_preds'][i], row['scibert_preds'][i], row['deberta_preds'][i],\n",
    "                              row['biomed_roberta_preds'][i], row['cs_roberta_preds'][i]]\n",
    "            \n",
    "            max_repeated = statistics.multimode(preds_data)\n",
    "            if len(max_repeated) != 1:\n",
    "                # Weighted avg\n",
    "                combined_preds_max[i] = random.choices(\n",
    "                    preds_data, weights=[model_weights['roberta'], model_weights['scibert'],\n",
    "                                         model_weights['deberta'], model_weights['biomed_roberta'], \n",
    "                                         model_weights['cs_roberta']],\n",
    "                    k=1)[0]\n",
    "                if combined_preds_max[i] == row['deberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'deberta'\n",
    "                elif combined_preds_max[i] == row['biomed_roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'biomed_roberta'\n",
    "                elif combined_preds_max[i] == row['roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'roberta'\n",
    "                elif combined_preds_max[i] == row['cs_roberta_preds'][i]:\n",
    "                    majority_model_prediction[i] = 'cs_roberta'\n",
    "                else:\n",
    "                    majority_model_prediction[i] = 'scibert'\n",
    "                majority_col_val[i] = 0\n",
    "            else:\n",
    "                combined_preds_max[i] = max_repeated[0]\n",
    "                if (row['deberta_preds'][i] == row['biomed_roberta_preds'][i]) and \\\n",
    "                (row['roberta_preds'][i] == row['biomed_roberta_preds'][i]) and \\\n",
    "                (row['roberta_preds'][i] == row['scibert_preds'][i]) and \\\n",
    "                (row['deberta_preds'][i] == row['scibert_preds'][i]) and \\\n",
    "                (row['cs_roberta_preds'][i] == row['scibert_preds'][i]):\n",
    "                    majority_model_prediction[i] = 'all'\n",
    "                else:\n",
    "                    majority_model_prediction[i] = 'majority'\n",
    "                majority_col_val[i] = 1\n",
    "        input_df.at[index,'preds'] = combined_preds_max\n",
    "        input_df.at[index, 'majority_model'] = majority_model_prediction\n",
    "        input_df.at[index, 'majority_col'] = majority_col_val\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f40ca2-c7a3-4dc6-8f08-a24943d60e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38943f-cce4-43c5-ab27-8442c3987ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7727375-d587-4776-ad77-b243d9490179",
   "metadata": {},
   "source": [
    "### Function calls to make the predictions and save the parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8cb173fb-5f4b-4571-816b-846ea782749f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after merging with roberta model (5000, 3)\n",
      "Data shape after merging with scibert model (5000, 4)\n",
      "Data shape after merging with deberta model (5000, 5)\n",
      "Data shape after merging with biomed_roberta model (5000, 6)\n",
      "Data shape after merging with cs_roberta model (5000, 7)\n",
      "CPU times: user 1min 30s, sys: 2.37 s, total: 1min 32s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merged_model_dev_predictions = merge_model_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d210dc0-7b12-4134-9ea7-77c3043554b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>roberta_preds</th>\n",
       "      <th>scibert_preds</th>\n",
       "      <th>deberta_preds</th>\n",
       "      <th>biomed_roberta_preds</th>\n",
       "      <th>cs_roberta_preds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12313</th>\n",
       "      <td>Phylogenetic networks are a generalization of ...</td>\n",
       "      <td>[Phylogenetic, networks, are, a, generalizatio...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>Prediction modelling is more closely aligned w...</td>\n",
       "      <td>[Prediction, modelling, is, more, closely, ali...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451</th>\n",
       "      <td>The heat transfer exhibits the flow of heat (t...</td>\n",
       "      <td>[The, heat, transfer, exhibits, the, flow, of,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4351</th>\n",
       "      <td>a common experience during superficial ultraso...</td>\n",
       "      <td>[a, common, experience, during, superficial, u...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22694</th>\n",
       "      <td>Code metadata Current code version v1.5.9 Perm...</td>\n",
       "      <td>[Code, metadata, Current, code, version, v1.5....</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "index                                                      \n",
       "12313  Phylogenetic networks are a generalization of ...   \n",
       "3172   Prediction modelling is more closely aligned w...   \n",
       "6451   The heat transfer exhibits the flow of heat (t...   \n",
       "4351   a common experience during superficial ultraso...   \n",
       "22694  Code metadata Current code version v1.5.9 Perm...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "12313  [Phylogenetic, networks, are, a, generalizatio...   \n",
       "3172   [Prediction, modelling, is, more, closely, ali...   \n",
       "6451   [The, heat, transfer, exhibits, the, flow, of,...   \n",
       "4351   [a, common, experience, during, superficial, u...   \n",
       "22694  [Code, metadata, Current, code, version, v1.5....   \n",
       "\n",
       "                                           roberta_preds  \\\n",
       "index                                                      \n",
       "12313  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3172   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6451   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4351   [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "22694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           scibert_preds  \\\n",
       "index                                                      \n",
       "12313  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3172   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6451   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4351   [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, ...   \n",
       "22694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           deberta_preds  \\\n",
       "index                                                      \n",
       "12313  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3172   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6451   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4351   [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "22694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    biomed_roberta_preds  \\\n",
       "index                                                      \n",
       "12313  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3172   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6451   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4351   [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "22694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        cs_roberta_preds  \n",
       "index                                                     \n",
       "12313  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3172   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6451   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4351   [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "22694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_dev_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24f51927-9e42-4f67-839a-f85d46c0d233",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#outputs = list(map(lambda x:x.invoke({\"input\":input_text}),structured_article_fos_chains))\n",
    "chain = structured_article_fos_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23d2182d-009b-40f6-ab2d-b154ebe09b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text_with_index = pd.DataFrame(merged_model_dev_predictions['text'])\n",
    "input_text_with_index_dict = input_text_with_index.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3110ea-1315-47bc-ad37-4486127d47d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def create_batches(input_text_index_dict):\n",
    "    length_of_data = len(list(input_text_index_dict.keys()))\n",
    "    batch_size = 100\n",
    "    nbatches = math.ceil(length_of_data/batch_size)\n",
    "    batches_of_keys = []\n",
    "    key_list = list(input_text_index_dict.keys())\n",
    "    for i in range(0,nbatches):\n",
    "        a_batch_of_keys = key_list[i*batch_size:(i+1)*batch_size]\n",
    "        batches_of_keys.append(a_batch_of_keys)\n",
    "    batches = list(map(lambda x:{key:input_text_index_dict.get(key,\"\")[\"text\"] for key in x},batches_of_keys))\n",
    "    return batches\n",
    "batches = create_batches(input_text_with_index_dict)\n",
    "batch_sizes = [len(b) for b in batches]\n",
    "batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c3a6d2-fce2-4544-87ef-baeb1623ee4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = batches[0]\n",
    "input_texts = [batch[key] for key in list(batch.keys())]\n",
    "results = chain.batch(input_texts)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cead54cf-6ecb-42cc-b1ed-db9610871ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def populate_field_of_study(batches,load_last_checkpt=None,checkpt=True):\n",
    "    LLM_MAX_LENGTH = os.getenv('LLM_MAX_LENGTH',16000)\n",
    "    LLM_MAX_LENGTH=int(LLM_MAX_LENGTH)\n",
    "    results_batches = []\n",
    "    rate_limit_delay = 5\n",
    "    checkpt_dict={}\n",
    "    i=0\n",
    "    while i < len(batches):\n",
    "        if load_last_checkpt is not None:\n",
    "            with open(load_last_checkpt,\"r+\") as f:\n",
    "                checkpt_dict=json.load(f)\n",
    "            f.close()\n",
    "            i = int(load_last_checkpt.strip('.json').split('_')[2])-1\n",
    "            print(f\"Loaded Last checkpointed batch {i+1}\")\n",
    "            results_batches = checkpt_dict[f\"Batch_{i+1}\"]\n",
    "            last_chekpt = load_last_checkpt.strip('.json')\n",
    "            load_last_checkpt = None\n",
    "            i+=1\n",
    "            if i<len(batches):\n",
    "                batch = batches[i]\n",
    "            else:\n",
    "                return results_batches, last_chekpt\n",
    "        else:\n",
    "            batch = batches[i]\n",
    "        print(f\"Processing Batch {i+1}\")\n",
    "        input_texts=[batch[key] for key in list(batch.keys())]\n",
    "        try:\n",
    "            results = chain.batch(input_texts)\n",
    "        except APITimeoutError:\n",
    "            checkpt_dict[f\"Batch_{i+1}\"] = results_batches\n",
    "            with open(f\"checkpt_batch_{i+1}.json\",\"w+\") as f:\n",
    "                json.dump(checkpt_dict,f)\n",
    "            f.close()\n",
    "            last_checkpt=f\"checkpt_batch_{i+1}\"\n",
    "        except RateLimitError:\n",
    "            delay = 30\n",
    "            print(f\"Rate Limit Error Encountered, sleeping for {delay} seconds\")\n",
    "            time.sleep(delay)\n",
    "            results = chain.batch(input_texts)\n",
    "            rate_limit_delay *=2\n",
    "            print(f\"Doubling rate limit delay between batches to {rate_limit_delay} seconds\")\n",
    "        except BadRequestError:\n",
    "            print(f\"Bad Request Error Hit, adjusting input text length to accomodate model context\")\n",
    "            encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "            encoded_input_texts = list(map(lambda x:encoding.encode(x),input_texts))\n",
    "            num_tokens_for_texts = list(map(lambda x:len(x),encoded_input_texts))\n",
    "            clipped_input_texts = []\n",
    "            for j,encoded_text in enumerate(encoded_input_texts):\n",
    "                num_tokens = len(encoded_text)\n",
    "                if num_tokens > LLM_MAX_LENGTH:\n",
    "                    print(f\"In Batch {i+1}, text {j+1} has {num_tokens} tokens, reducing it down to {LLM_MAX_LENGTH}\")\n",
    "                    clipped_input_texts.append(encoding.decode(encoded_text[:LLM_MAX_LENGTH]))\n",
    "                else:\n",
    "                    #print(f\"In Batch {i+1}, text {j+1} has {num_tokens} tokens, NOT reducing the tokens\")\n",
    "                    clipped_input_texts.append(encoding.decode(encoded_text))\n",
    "            try:\n",
    "                results = chain.batch(clipped_input_texts)\n",
    "            except RateLimitError:\n",
    "                delay = 30\n",
    "                print(f\"Rate Limit Error Encountered, sleeping for {delay} seconds\")\n",
    "                time.sleep(delay)\n",
    "                results = chain.batch(clipped_input_texts)\n",
    "                rate_limit_delay *=2\n",
    "                print(f\"Doubling rate limit delay between batches to {rate_limit_delay} seconds\")\n",
    "        batch_result_dict = {key:results[i] for i,key in enumerate(list(batch.keys()))}\n",
    "        results_batches.append(batch_result_dict)\n",
    "        if checkpt:\n",
    "            checkpt_dict[f\"Batch_{i+1}\"] = results_batches\n",
    "            with open(f\"checkpt_batch_{i+1}.json\",\"w+\") as f:\n",
    "                json.dump(checkpt_dict,f)\n",
    "            f.close()\n",
    "            last_checkpt=f\"checkpt_batch_{i+1}\"\n",
    "        if i+2<=len(batches):\n",
    "            print(f\"Sleeping for {rate_limit_delay} seconds before processing batch {i+2}\")\n",
    "        time.sleep(rate_limit_delay)\n",
    "        if i<len(batches):\n",
    "            i+=1\n",
    "    return results_batches, last_chekpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8514e-0c37-47ac-a3ce-4143cec62a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "last_checkpt=None\n",
    "results_batches, last_checkpt = populate_field_of_study(batches,load_last_checkpt=f\"{last_checkpt}.json\" if last_checkpt is not None else None)\n",
    "last_checkpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c44e2-5ba8-473a-813c-a8cb51c5f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_dev_predictions[['major_field_of_study','sub_areas_within_major_field_of_study','allied_fields_of_study']] = merged_model_dev_predictions.apply(lambda x:populate_field_of_study(x),axis=1,result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b325f-8f00-4a5d-89ed-2cc016c28fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model_dev_predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d8a20-2995-482c-9331-17a29eef60ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "majority_vote_df = get_max_repeated_pred(merged_model_dev_predictions)\n",
    "majority_vote_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37934e13-041a-44ad-810a-b94d31975a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote_df[['preds']].to_parquet('predictions_four_models.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07a6ea-29d6-420f-930b-12c5a730f7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "majority_vote_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1a1ca-a626-48ea-8b6c-85020f0acf2a",
   "metadata": {},
   "source": [
    "# End here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b7721-2cbd-4408-a80b-b389802b76a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d946faf-f677-4bb8-b98f-b46cf6d8be69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae70220-fbe1-40f3-8bc4-64c2469496cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e24b0a7e-0ffb-40ae-8f93-d9acf7699f4d",
   "metadata": {},
   "source": [
    "### Finding the stats about the majority column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a28361-a0d4-40e2-869a-79a89756db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_stats(row):\n",
    "    stats_dict = row['stats']\n",
    "    out_dict = {}\n",
    "    out_keys =  ['roberta', 'scibert', 'deberta', 'biomed_roberta',  'all']\n",
    "    out_dict = {key:stats_dict[key] if key in list(stats_dict.keys()) else 0 for key in out_keys}\n",
    "    \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d853f-687e-4707-9232-a891af78d934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "majority_vote_df['stats'] = majority_vote_df[['majority_model']].map(lambda x: Counter(x))\n",
    "# majority_vote_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51091ee3-4c35-4c7c-a2a6-05a6b2dd56ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df = majority_vote_df[['tokens', 'stats', 'majority_col']]\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211da002-bae6-43bd-aba4-cf698c1edd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['len_tokens'] = stats_df['tokens'].map(lambda x : len(x))\n",
    "stats_df.drop(columns=['tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169117c-bf02-4455-b53c-8ecc50825aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['sum_majority'] = stats_df['majority_col'].map(lambda x : sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188936a8-b6ba-42c1-9faf-3ad1d0fa61da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['sum_majority_not_all'] = stats_df['sum_majority'] - stats_df['all']\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54fe52-7054-466a-9001-0a7ac59a96f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['pct_majority'] = 100 * stats_df['sum_majority']/stats_df['len_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8354796-f934-4110-a11e-0e5c7229fc82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151631eb-d4fc-442f-af7b-bd3489f50175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df[['roberta', 'scibert', 'deberta', 'biomed_roberta',  'all']] = stats_df.apply(\n",
    "    lambda x : get_model_stats(x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25b1aa-c0ce-497e-8a3b-fb0934a0a5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e999c-66c5-45f1-9b93-f916e35c2deb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['pct_majority'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a06169-37cb-4c55-9436-5b6dd2e3dbe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['roberta'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8bc9e-fdd9-4c2c-a289-680464bb7c47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['scibert'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f7dbb-6fb5-4fe0-82b0-2fa1acb1849c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['deberta'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf608bdc-f46c-4224-9dd9-cc69d2d5a045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['biomed_roberta'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335d9a5-2b3f-4c5b-86ad-64b22253115f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df['all'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0fa43-53ac-4a07-967e-51e44b8b9c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33971d40-bdff-495e-9ab2-5b8d7e1d72db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86752cdc-4dc5-4848-8c63-549c13286e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87cd608c-8f14-434f-b32e-ee11b9a142f0",
   "metadata": {},
   "source": [
    "# For local runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d6fdb-4b9c-45cc-964b-3bfb66538e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"/Users/gayatri/Documents/Gayatri/US/Self projects/AI Competition/DAGPAP24/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21ceaf-d851-4d83-8f31-e05c026eb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_parquet(base_folder + os.sep + 'dev_data.parquet', engine=\"fastparquet\")\n",
    "print(dev_df.shape)\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f5441-2828-4747-9758-4a3cf718f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac76cf-982f-4d2a-9dfd-5913a2ecabfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7305abef-0565-48c5-8cc2-d28b189a437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = dev_df.copy(deep=True)\n",
    "\n",
    "for model in contesting_models:\n",
    "    model_df = pd.read_parquet(base_folder + os.sep + f'predictions_{model}.parquet', engine=\"fastparquet\")\n",
    "    model_df.rename(columns={'preds': f'{model}_preds'}, inplace=True)\n",
    "\n",
    "    merged = merged.merge(model_df, how='inner', left_index=True, right_index=True)\n",
    "    print(f\"Data shape after merging with {model} model {merged.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d75f13-2f46-4dca-9172-4459a974167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6293467-7e76-4c49-a78f-211674741910",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_pred_df = get_max_repeated_pred(merged)\n",
    "print(max_pred_df.shape)\n",
    "max_pred_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71dd3e-88e6-45e5-bb95-ce2d5db1e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.88/(0.87+0.88+0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124c868-e0af-4283-8c06-af1c8c1563b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4173c-87c1-4ea8-88b9-b7f131fed1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pred_df[['preds']].to_parquet(base_folder + os.sep + 'predictions_three_models_combined.parquet') # , engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493d2de-5050-42ba-bf2e-2c8a4b7be95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10137373-3487-468a-a7ee-d85430e0d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(base_folder + os.sep + 'dev_majority_model_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7ff62-6724-460d-ad2f-31de66a6889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_parquet(base_folder + os.sep + 'dev_majority_model_preds.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397aa1a-d080-4744-a34a-ca9807b20e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325a81e-3f8f-45e8-b564-06ac3dee78d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
