{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec8ad65-548e-4f5e-b636-7a87c3e57b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7d4c6ea-5494-45ec-a49b-837d70130e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import ast\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a6234",
   "metadata": {},
   "source": [
    "0 for 'human'\n",
    "1 for 'NLTK_synonym_replacement'\n",
    "2 for 'chatgpt'\n",
    "3 for 'summarized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d49c8b07-e738-4f81-94ed-d69c2e176335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {\"human\":0,\"NLTK_synonym_replacement\":1,\"chatgpt\":2,\"summarized\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b16050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pq.ParquetDataset('s3://dagpapsubmission/data/data_train_data.parquet', filesystem=s3).read_pandas().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72af259e-e72e-4328-9619-d1b34b974624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12313</th>\n",
       "      <td>Phylogenetic networks are a generalization of ...</td>\n",
       "      <td>b'[\"Phylogenetic\",\"networks\",\"are\",\"a\",\"genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>Prediction modelling is more closely aligned w...</td>\n",
       "      <td>b'[\"Prediction\",\"modelling\",\"is\",\"more\",\"close...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451</th>\n",
       "      <td>The heat transfer exhibits the flow of heat (t...</td>\n",
       "      <td>b'[\"The\",\"heat\",\"transfer\",\"exhibits\",\"the\",\"f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4351</th>\n",
       "      <td>a common experience during superficial ultraso...</td>\n",
       "      <td>b'[\"a\",\"common\",\"experience\",\"during\",\"superfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22694</th>\n",
       "      <td>Code metadata Current code version v1.5.9 Perm...</td>\n",
       "      <td>b'[\"Code\",\"metadata\",\"Current\",\"code\",\"version...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "index                                                      \n",
       "12313  Phylogenetic networks are a generalization of ...   \n",
       "3172   Prediction modelling is more closely aligned w...   \n",
       "6451   The heat transfer exhibits the flow of heat (t...   \n",
       "4351   a common experience during superficial ultraso...   \n",
       "22694  Code metadata Current code version v1.5.9 Perm...   \n",
       "\n",
       "                                                  tokens  \n",
       "index                                                     \n",
       "12313  b'[\"Phylogenetic\",\"networks\",\"are\",\"a\",\"genera...  \n",
       "3172   b'[\"Prediction\",\"modelling\",\"is\",\"more\",\"close...  \n",
       "6451   b'[\"The\",\"heat\",\"transfer\",\"exhibits\",\"the\",\"f...  \n",
       "4351   b'[\"a\",\"common\",\"experience\",\"during\",\"superfi...  \n",
       "22694  b'[\"Code\",\"metadata\",\"Current\",\"code\",\"version...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df = pq.ParquetDataset('s3://dagpapsubmission/data/data_dev_data.parquet', filesystem=s3).read_pandas().to_pandas()\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5a6d20-7fe7-43ee-9c4a-b3b9b48ef33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df[\"tokens_label_lst\"] = train_df.token_label_ids.map(lambda x:ast.literal_eval(x.decode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21ef84f2-7f9c-474a-8978-9c45fe264780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing = train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd921ec-eba1-43ba-8f0f-4d0044356953",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_label_lst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15096</th>\n",
       "      <td>{0: 2819, 1: 547, 3: 455}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428</th>\n",
       "      <td>{1: 625, 0: 3705}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>{2: 477, 0: 2135, 1: 827}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5826</th>\n",
       "      <td>{0: 2957, 2: 494, 3: 368}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>{0: 4533, 3: 815, 1: 704, 2: 352}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        tokens_label_lst\n",
       "index                                   \n",
       "15096          {0: 2819, 1: 547, 3: 455}\n",
       "14428                  {1: 625, 0: 3705}\n",
       "2144           {2: 477, 0: 2135, 1: 827}\n",
       "5826           {0: 2957, 2: 494, 3: 368}\n",
       "1452   {0: 4533, 3: 815, 1: 704, 2: 352}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing[['tokens_label_lst']].map(lambda x: Counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294fcefb-809a-4969-a2c5-3aff633b96f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428cbcf2-3111-4054-bd51-3e8966370113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ec7fec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e6e8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_transformed = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d311e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_label_ids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15096</th>\n",
       "      <td>Across the world, Emergency Departments are fa...</td>\n",
       "      <td>b'[[0,3779,\"human\"],[3780,7601,\"NLTK_synonym_r...</td>\n",
       "      <td>b'[\"Across\",\"the\",\"world,\",\"Emergency\",\"Depart...</td>\n",
       "      <td>b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428</th>\n",
       "      <td>lung Crab is the in the lead make of cancer-re...</td>\n",
       "      <td>b'[[0,4166,\"NLTK_synonym_replacement\"],[4167,2...</td>\n",
       "      <td>b'[\"lung\",\"Crab\",\"is\",\"the\",\"in\",\"the\",\"lead\",...</td>\n",
       "      <td>b'[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>The number of osteoporotic fractures, particul...</td>\n",
       "      <td>b'[[0,3264,\"chatgpt\"],[3265,17179,\"human\"],[17...</td>\n",
       "      <td>b'[\"The\",\"number\",\"of\",\"osteoporotic\",\"fractur...</td>\n",
       "      <td>b'[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5826</th>\n",
       "      <td>The COVID-19 pandemic has spread to every coun...</td>\n",
       "      <td>b'[[0,3666,\"human\"],[3667,6954,\"chatgpt\"],[695...</td>\n",
       "      <td>b'[\"The\",\"COVID-19\",\"pandemic\",\"has\",\"spread\",...</td>\n",
       "      <td>b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>Endophytic fungi live a significant part of th...</td>\n",
       "      <td>b'[[0,10489,\"human\"],[10490,12000,\"summarized\"...</td>\n",
       "      <td>b'[\"Endophytic\",\"fungi\",\"live\",\"a\",\"significan...</td>\n",
       "      <td>b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "index                                                      \n",
       "15096  Across the world, Emergency Departments are fa...   \n",
       "14428  lung Crab is the in the lead make of cancer-re...   \n",
       "2144   The number of osteoporotic fractures, particul...   \n",
       "5826   The COVID-19 pandemic has spread to every coun...   \n",
       "1452   Endophytic fungi live a significant part of th...   \n",
       "\n",
       "                                             annotations  \\\n",
       "index                                                      \n",
       "15096  b'[[0,3779,\"human\"],[3780,7601,\"NLTK_synonym_r...   \n",
       "14428  b'[[0,4166,\"NLTK_synonym_replacement\"],[4167,2...   \n",
       "2144   b'[[0,3264,\"chatgpt\"],[3265,17179,\"human\"],[17...   \n",
       "5826   b'[[0,3666,\"human\"],[3667,6954,\"chatgpt\"],[695...   \n",
       "1452   b'[[0,10489,\"human\"],[10490,12000,\"summarized\"...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "15096  b'[\"Across\",\"the\",\"world,\",\"Emergency\",\"Depart...   \n",
       "14428  b'[\"lung\",\"Crab\",\"is\",\"the\",\"in\",\"the\",\"lead\",...   \n",
       "2144   b'[\"The\",\"number\",\"of\",\"osteoporotic\",\"fractur...   \n",
       "5826   b'[\"The\",\"COVID-19\",\"pandemic\",\"has\",\"spread\",...   \n",
       "1452   b'[\"Endophytic\",\"fungi\",\"live\",\"a\",\"significan...   \n",
       "\n",
       "                                         token_label_ids  \n",
       "index                                                     \n",
       "15096  b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...  \n",
       "14428  b'[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1...  \n",
       "2144   b'[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2...  \n",
       "5826   b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...  \n",
       "1452   b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e75586a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "train_df_transformed[\"annotation_lst\"] = train_df_transformed.annotations.map(lambda x:ast.literal_eval(x.decode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a60a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_transformed[\"tokens_label_lst\"] = train_df_transformed.token_label_ids.map(lambda x:ast.literal_eval(x.decode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56b40935",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_transformed[\"tokens_lst\"] = train_df_transformed.tokens.map(lambda x:ast.literal_eval(x.decode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "868fb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_lst(row):\n",
    "    annotation = ast.literal_eval(row[\"annotations\"].decode())\n",
    "    tokens_label = ast.literal_eval(row[\"token_label_ids\"].decode())\n",
    "    tokens = ast.literal_eval(row[\"tokens\"].decode())\n",
    "    return annotation,tokens_label,tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03c45c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_transformed.drop(columns=[\"annotation_lst\",\"tokens_label_lst\",\"tokens_lst\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c38681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_transformed[[\"annotation\",\"token_labels\",\"token_lst\"]] = train_df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc06213a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_label_ids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15096</th>\n",
       "      <td>b'[[0,3779,\"human\"],[3780,7601,\"NLTK_synonym_r...</td>\n",
       "      <td>b'[\"Across\",\"the\",\"world,\",\"Emergency\",\"Depart...</td>\n",
       "      <td>b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428</th>\n",
       "      <td>b'[[0,4166,\"NLTK_synonym_replacement\"],[4167,2...</td>\n",
       "      <td>b'[\"lung\",\"Crab\",\"is\",\"the\",\"in\",\"the\",\"lead\",...</td>\n",
       "      <td>b'[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>b'[[0,3264,\"chatgpt\"],[3265,17179,\"human\"],[17...</td>\n",
       "      <td>b'[\"The\",\"number\",\"of\",\"osteoporotic\",\"fractur...</td>\n",
       "      <td>b'[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5826</th>\n",
       "      <td>b'[[0,3666,\"human\"],[3667,6954,\"chatgpt\"],[695...</td>\n",
       "      <td>b'[\"The\",\"COVID-19\",\"pandemic\",\"has\",\"spread\",...</td>\n",
       "      <td>b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>b'[[0,10489,\"human\"],[10490,12000,\"summarized\"...</td>\n",
       "      <td>b'[\"Endophytic\",\"fungi\",\"live\",\"a\",\"significan...</td>\n",
       "      <td>b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             annotations  \\\n",
       "index                                                      \n",
       "15096  b'[[0,3779,\"human\"],[3780,7601,\"NLTK_synonym_r...   \n",
       "14428  b'[[0,4166,\"NLTK_synonym_replacement\"],[4167,2...   \n",
       "2144   b'[[0,3264,\"chatgpt\"],[3265,17179,\"human\"],[17...   \n",
       "5826   b'[[0,3666,\"human\"],[3667,6954,\"chatgpt\"],[695...   \n",
       "1452   b'[[0,10489,\"human\"],[10490,12000,\"summarized\"...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "15096  b'[\"Across\",\"the\",\"world,\",\"Emergency\",\"Depart...   \n",
       "14428  b'[\"lung\",\"Crab\",\"is\",\"the\",\"in\",\"the\",\"lead\",...   \n",
       "2144   b'[\"The\",\"number\",\"of\",\"osteoporotic\",\"fractur...   \n",
       "5826   b'[\"The\",\"COVID-19\",\"pandemic\",\"has\",\"spread\",...   \n",
       "1452   b'[\"Endophytic\",\"fungi\",\"live\",\"a\",\"significan...   \n",
       "\n",
       "                                         token_label_ids  \n",
       "index                                                     \n",
       "15096  b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...  \n",
       "14428  b'[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1...  \n",
       "2144   b'[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2...  \n",
       "5826   b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...  \n",
       "1452   b'[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "08204743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: {'Total Text tokens': 3822, 'Total unique Text tokens': 1265, 'Unqiue Text Tokens not in Token Vocab': 1, 'Unique tokens in vocab not in text tokens': 0}\n"
     ]
    }
   ],
   "source": [
    "labels = ast.literal_eval(train_df_transformed.token_label_ids.tolist()[0].decode())\n",
    "tokens = ast.literal_eval(train_df_transformed.tokens.tolist()[0].decode())\n",
    "text = train_df_transformed.text.tolist()[0].split(' ')\n",
    "tokens_not_in_text = [tok for tok in tokens if tok not in text]\n",
    "text_not_in_tokens = [t for t in text if t not in tokens]\n",
    "summ_dict = {\"Total Text tokens\":len(text), \n",
    "             \"Total unique Text tokens\": len(set(text)), \n",
    "             \"Unqiue Text Tokens not in Token Vocab\": len(set(text_not_in_tokens)),\n",
    "             \"Unique tokens in vocab not in text tokens\": len(set(tokens_not_in_text))}\n",
    "print(f\"Summary: {summ_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b595da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_str_human = train_df_transformed.text.tolist()[0][0:3779]\n",
    "text_str_nltk = train_df_transformed.text.tolist()[0][3780:7601]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3332dff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Across the world, Emergency Departments are facing increasing challenges in delivering high quality patient care due to growing patient numbers and limited hospital resources. Excessive patient waiting time, slow investigation, and delays in making disposition decisions are some causes of overcrowding in Emergency Departments . One way to address the issue of overcrowding is to closely examine turnaround times for all diagnostic tests. A reduction of diagnostic testing time by having only one sample tube for chemistry, hematology and coagulation measurements and an instrument capable of analyzing all these analytes simultaneously can help reduce the time of triaging and or diagnosing by health care professionals. We evaluated a new antithrombotic formulation to measure biochemical, hematological, and d -dimer measurements from a single blood collection tube. Therefore, to analyze hematological, biochemical, and d -dimer in the same blood sample, it is necessary to develop a novel formulation with properties that simultaneously prevent coagulation and cell activation and are not interfering with chemical/immunochemical measurements. Moreover, the novel formulation needs to stabilize and preserve blood cells, including abnormal cells that may have increased activity in different diseases. In the routine hematology laboratory, ethylenediaminetetra-acetic acid (EDTA) is the most used anticoagulant. EDTA is an irreversible calcium chelator with calcium-complexing capacity, which stabilizes the fluidity of whole blood. The disadvantage of EDTA-anticoagulated blood is the time-dependent swelling of platelets and an EDTA-dependent introduction of pseudothrombocytopenia in samples from some individuals . The EDTA-dependent pseudothrombocytopenia is a rare in vitro phenomenon (around 0.1 % in the general population), which is caused mostly by the presence of EDTA-dependent antiplatelet antibodies, which recognize GPIIb-IIIa receptors on platelet membranes. These antibodies trigger platelet activation/aggregation in vitro , which finally lead to a spuriously decreased platelet count . In addition, due to the divalent cation binding, EDTA is not suitable for a variety of chemical and immunochemical analysis. Sodium citrate is used in clinical laboratories as an effective anticoagulant to assess the coagulation cascade as it reversibly chelates calcium. The citrate ion chelates free calcium ions by forming calcium citrate complexes and thus disrupting the blood coagulation mechanism. Calcium is a necessary co-factor to several steps in the blood coagulation cascade, which is required for both intrinsic and extrinsic coagulation pathways . Moreover, platelet activation depends on free calcium ion fluxes . Furthermore, in citrated plasma, the d -dimer, a classic coagulation biomarker, is conveniently measured together with the other coagulation factors, but d -dimer can also be measured in heparinized plasma . Lithium heparin is the most used anticoagulant for routine clinical chemistry/immunochemistry analysis. Heparin prevents blood from clotting as the unique pentasaccharide sequence binds to antithrombin-III, which is a plasma protein. The formation of the antithrombin-protease complex is irreversible, naturally slow and an inefficient reaction. However, the process can be rapidly increased up to 1000-fold due to unique high-affinity towards the antithrombin heparin-binding domain . The heparin-antithrombin-III complex inactivates several coagulation enzymes including thrombin and factors XIa, Xa, IXa. In addition, thrombin, and factor Xa are the most sensitive factors to the effects of heparin-antithrombin-III complex inhibition . Several studies have demonstrated in vitro direct platelet aggregation induced by heparin '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_str_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8e9b0efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In addition, GAO et Camellia State. reported a refreshing mechanics of platelet activating via GPIIb/IIIa mediated outside-in signaling by Lipo Hepin. Due to this blood platelet activating Liquaemin is not suited for haematological depth psychology. various option decoagulant conceptualization have been key such as hirudin and atomic number 12 sulfate. The most victimized assess for the decoagulant action of hirudin are the thrombin clip and the aPTT. Van Resburg et Al. read thrombocyte functionality by light transmitting aggregometry to judge the commercially useable citrate and hirudinized stock solicitation vacuum tube by victimisation unlike agonist. The study dissolve that the commercially usable hirudin tube do not provide a honorable anticoagulant core liken to the commercially useable Na citrate vacuum tube for thrombocyte functionality work. However, there is a clock time restriction of Little Joe h consort with the use of atomic number 11 citrate electron tube. Magnesium sulfate is commercially uncommitted as ThromboExact-Monovette (Sarstedt), but it was ne'er infix into the quotidian haematology laboratory. magnesium sulphate in effect boot out EDTA-induced pseudothrombocytopenia , although blood platelet counts, and volume look to be underestimate liken to EDTA-anticoagulated rakehell. Therefore, further growth and normalisation of lineage aggregation thermionic valve anticoagulants is necessity. The rationale and object of this study are to pass judgment the new prepare antithrombotic (anticoagulant and antiplatelet) blood collection tube and present its electric potential for versatile lineage agent measuring. The blood hemostasis is kept up by endothelial cells, which produce and tone ending azotic oxide and prostacyclin to the lm of rip vessels to conquer blood platelet and white blood cell energizing and collection. Prostacyclin is a chief product of arachidonic acerbic and the most powerful endogenic inhibitor of thrombocyte assemblage let on. Moreover, prostacyclin is always generate by the vascular bulwark and let go of by the lung into the arterial circulation. Therefore, go around platelets are bring out constantly to prostacyclin stimulus. Due to this, the blood platelet aggregability in vivo is assure. Prostacyclin has an matter to possible for clinical lotion in shape where heighten blood platelet assemblage is ask or to gain biocompatibility of extracorporeal circulation system. The challenge and disfavour of prostacyclin clinical exercise is its little half-life, inside 30 Min of presidency in vivo. Therefore, one of the most of import avenues in sanative search has focalize on ascertain more chemically unchanging prostacyclin analogs. We have rise a new heparin-based antithrombotic preparation by using a synthetic. parallel of prostacyclin PGI ii Iloprost (Cas add up 78919–13-8, InChIKey: HIFJCPQKFCZDDL-ACWOEMLNSA-N). The accusative of our meditate was to assess the performance of this new antithrombotic expression (HEP-ILOP) vacuum tube in comparing with routine banner line ingathering vacuum tube by victimisation stock clinical instruments to measure dissimilar biochemical, haematological and d -dimer mensuration in blood, compile from patient role with respective clinical conditions. Materials and method acting The study was approved by the Ethical committee in the great area of Denmark. (H-20010519), which adjust to the declaration of capital of Finland. Furthermore, the work was O.K. by the danish Data Protection delegacy. Informed go for was hold from all affected role. developing of the novel heparin-iloprost-based antithrombotic preparation blood ingathering subway The developmental tube was a Li heparin subway system preloaded with a liquified manakin of Iloprost (Sigma-Aldrich) fade out in ethyl alcohol and nought\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_str_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7192a-c4b0-4f3a-a619-1b9d1e114f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# For running locally\n",
    "train_df = pd.read_parquet('/Users/gayatri/Downloads/train_data.parquet', engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf18e8-aa41-4617-843c-9879e18a0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ea3b8cf-3f61-47d6-abef-387eb26ba15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_df.iloc[0]['tokens']))\n",
    "print(type(train_df.iloc[0]['token_label_ids']))\n",
    "print(type(train_df.iloc[0]['annotations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7c35d-2144-4b89-aac5-dc7fa158b354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d668fd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64291492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c820e30-138e-44cf-af7b-497f8a5d3c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_label_ids</th>\n",
       "      <th>len_tokens</th>\n",
       "      <th>len_token_ids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15096</th>\n",
       "      <td>Across the world, Emergency Departments are fa...</td>\n",
       "      <td>[[0, 3779, human], [3780, 7601, NLTK_synonym_r...</td>\n",
       "      <td>[Across, the, world,, Emergency, Departments, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3821</td>\n",
       "      <td>3821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428</th>\n",
       "      <td>lung Crab is the in the lead make of cancer-re...</td>\n",
       "      <td>[[0, 4166, NLTK_synonym_replacement], [4167, 2...</td>\n",
       "      <td>[lung, Crab, is, the, in, the, lead, make, of,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>4330</td>\n",
       "      <td>4330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>The number of osteoporotic fractures, particul...</td>\n",
       "      <td>[[0, 3264, chatgpt], [3265, 17179, human], [17...</td>\n",
       "      <td>[The, number, of, osteoporotic, fractures,, pa...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>3439</td>\n",
       "      <td>3439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5826</th>\n",
       "      <td>The COVID-19 pandemic has spread to every coun...</td>\n",
       "      <td>[[0, 3666, human], [3667, 6954, chatgpt], [695...</td>\n",
       "      <td>[The, COVID-19, pandemic, has, spread, to, eve...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3819</td>\n",
       "      <td>3819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>Endophytic fungi live a significant part of th...</td>\n",
       "      <td>[[0, 10489, human], [10490, 12000, summarized]...</td>\n",
       "      <td>[Endophytic, fungi, live, a, significant, part...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>6404</td>\n",
       "      <td>6404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "index                                                      \n",
       "15096  Across the world, Emergency Departments are fa...   \n",
       "14428  lung Crab is the in the lead make of cancer-re...   \n",
       "2144   The number of osteoporotic fractures, particul...   \n",
       "5826   The COVID-19 pandemic has spread to every coun...   \n",
       "1452   Endophytic fungi live a significant part of th...   \n",
       "\n",
       "                                             annotations  \\\n",
       "index                                                      \n",
       "15096  [[0, 3779, human], [3780, 7601, NLTK_synonym_r...   \n",
       "14428  [[0, 4166, NLTK_synonym_replacement], [4167, 2...   \n",
       "2144   [[0, 3264, chatgpt], [3265, 17179, human], [17...   \n",
       "5826   [[0, 3666, human], [3667, 6954, chatgpt], [695...   \n",
       "1452   [[0, 10489, human], [10490, 12000, summarized]...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "15096  [Across, the, world,, Emergency, Departments, ...   \n",
       "14428  [lung, Crab, is, the, in, the, lead, make, of,...   \n",
       "2144   [The, number, of, osteoporotic, fractures,, pa...   \n",
       "5826   [The, COVID-19, pandemic, has, spread, to, eve...   \n",
       "1452   [Endophytic, fungi, live, a, significant, part...   \n",
       "\n",
       "                                         token_label_ids  len_tokens  \\\n",
       "index                                                                  \n",
       "15096  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        3821   \n",
       "14428  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...        4330   \n",
       "2144   [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...        3439   \n",
       "5826   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        3819   \n",
       "1452   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        6404   \n",
       "\n",
       "       len_token_ids  \n",
       "index                 \n",
       "15096           3821  \n",
       "14428           4330  \n",
       "2144            3439  \n",
       "5826            3819  \n",
       "1452            6404  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify if len(tokens) == len(token_label_ids) in each row\n",
    "train_df['len_tokens'] = train_df['tokens'].map(len)\n",
    "train_df['len_token_ids'] = train_df['token_label_ids'].map(len)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d52b33d-5825-4d32-b51c-c8ab3d416124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_label_ids</th>\n",
       "      <th>len_tokens</th>\n",
       "      <th>len_token_ids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, annotations, tokens, token_label_ids, len_tokens, len_token_ids]\n",
       "Index: []"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['len_tokens'] != train_df['len_token_ids']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c8269c-a2da-4b34-b59b-788a47fabe4b",
   "metadata": {},
   "source": [
    "#### Verified that len(tokens) = len(token_label_ids) in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e2d51e-0258-4c38-8877-abe727030f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44ac12a4-a8c1-4f4c-8b57-0073e0799fb1",
   "metadata": {},
   "source": [
    "#### Splitting each row of data into multiple rows based on the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9be3d30b-939d-4bc7-a7fd-a7336f477c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_rows_on_annotation(input_df):\n",
    "    transformed_rows = []\n",
    "    for index, row in input_df.iterrows():\n",
    "        # print(index)\n",
    "        len_token = len(row['tokens'])\n",
    "        for new_annot in row['annotations']:\n",
    "            row_dict = {}\n",
    "            row_dict['doc_id'] = index\n",
    "            # row_dict['doc_text'] = row['text']\n",
    "            row_dict['tokens'] = row['tokens'][new_annot[0]:new_annot[1]]\n",
    "            row_dict['token_label_ids'] = row['token_label_ids'][new_annot[0]:new_annot[1]]\n",
    "            row_dict['annotation'] = new_annot[2]\n",
    "            row_dict['start_id'] = new_annot[0]\n",
    "            row_dict['end_id'] = new_annot[1]\n",
    "            row_dict['token_length'] = len(row_dict['tokens'])\n",
    "            if row_dict['token_length'] == 0:\n",
    "                row_dict['tokens'] = None\n",
    "            row_dict['token_label_id_length'] = len(row_dict['token_label_ids'])\n",
    "            if row_dict['token_label_id_length'] == 0:\n",
    "                row_dict['token_label_ids'] = None\n",
    "            \n",
    "            row_dict['exp_token_length'] = new_annot[1] - new_annot[0]\n",
    "            row_dict['doc_token_length'] = len_token\n",
    "            \n",
    "            if row_dict['tokens'] is not None:\n",
    "                unique_ids = set(row_dict['token_label_ids'])\n",
    "                row_dict['len_unique_token_ids'] = len(unique_ids)\n",
    "                if row_dict['len_unique_token_ids'] == 1:\n",
    "                    row_dict['unique_token_id'] = unique_ids.pop()\n",
    "                else:\n",
    "                    row_dict['unique_token_id'] = unique_ids\n",
    "            else:\n",
    "                row_dict['unique_token_id'] = None\n",
    "                row_dict['len_unique_token_ids'] = 0\n",
    "\n",
    "            if row_dict['annotation'] == \"human\":\n",
    "                row_dict['expected_token_label_id'] = 0\n",
    "            elif row_dict['annotation'] == \"NLTK_synonym_replacement\":\n",
    "                row_dict['expected_token_label_id'] = 1\n",
    "            elif row_dict['annotation'] == \"chatgpt\":\n",
    "                row_dict['expected_token_label_id'] = 2\n",
    "            elif row_dict['annotation'] == \"summarized\":\n",
    "                row_dict['expected_token_label_id'] = 3\n",
    "            else:\n",
    "                row_dict['expected_token_label_id'] = None                \n",
    "                 \n",
    "            transformed_rows.append(row_dict)\n",
    "            \n",
    "    return pd.DataFrame(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af128c7-d89c-4c38-9512-6f0c73387ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "985ef8e4-fd43-4d35-b0f5-f5d5ad082f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_checks(transformed_df):\n",
    "    print(\"Total rows in the df\", transformed_df.shape)\n",
    "    print()\n",
    "    # Find the number of Nulls\n",
    "    print(\"Null check\")\n",
    "    display(transformed_df.isna().sum().reset_index().rename(columns={'index':'column_names', 0:'null_count'}))\n",
    "    print()\n",
    "    \n",
    "    # Number of rows with token_length != token_label_id_length\n",
    "    print(\"Mismatches where tokens and token_label_ids in input file do not have the same size\")\n",
    "    display(transformed_df[transformed_df['token_length'] != transformed_df['token_label_id_length']])\n",
    "    print()\n",
    "    \n",
    "    # Find the number of rows with len_unique_token_ids > 1\n",
    "    print(\"Number of rows with more than one unique token id\", transformed_df[transformed_df['len_unique_token_ids'] > 1].shape[0])\n",
    "    print()\n",
    "    print(\"Number of rows with only one unique token id\", transformed_df[transformed_df['len_unique_token_ids'] == 1].shape[0])\n",
    "    \n",
    "    # Number of rows with unique_token_id != expected_token_label_id\n",
    "    print(\"Rows with only one unique token id but the token id does not match with the expected token label id\")\n",
    "    display(transformed_df[(transformed_df['len_unique_token_ids'] == 1) & \n",
    "            (transformed_df['unique_token_id'] != transformed_df['expected_token_label_id'])])\n",
    "    print()\n",
    "\n",
    "    # Label distribution for the data with unique labels\n",
    "    print(\"Label distribution for the data with unique labels\")\n",
    "    display(transformed_df[transformed_df['len_unique_token_ids'] == 1][['annotation', 'expected_token_label_id']].value_counts().reset_index())\n",
    "    print()\n",
    "    \n",
    "    # Label distribution for all data\n",
    "    print(\"Label distribution for all the data\")\n",
    "    display(transformed_df[['annotation', 'expected_token_label_id']].value_counts().reset_index())\n",
    "    print()\n",
    "\n",
    "    print(\"Token id distribution for unique labels\")\n",
    "    display(transformed_df[transformed_df['len_unique_token_ids'] == 1][['annotation', 'unique_token_id']].value_counts().reset_index())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "690224fe-bd9f-4625-a57a-d964679bad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fractions of token length in each category - find actual fraction and expected fraction\n",
    "def find_fractional_length(df):\n",
    "    no_null_df = df.dropna(subset=['tokens'], ignore_index=True)\n",
    "    no_null_stats = no_null_df.groupby(['doc_id', 'annotation', 'expected_token_label_id', 'doc_token_length'\n",
    "                                       ])[['token_label_id_length', 'exp_token_length']].sum().reset_index()\n",
    "    max_end_id = no_null_df.groupby(['doc_id', 'annotation', 'expected_token_label_id', 'doc_token_length'\n",
    "                                       ])[['end_id']].max().reset_index()\n",
    "    no_null_stats = no_null_stats.merge(max_end_id, how='inner', on=['doc_id', 'annotation', 'expected_token_label_id', \n",
    "                                                                     'doc_token_length'])\n",
    "    \n",
    "    no_null_stats['exp_doc_length'] = no_null_stats['end_id'] + 1\n",
    "    no_null_stats['token_length_frac'] = no_null_stats['token_label_id_length']/ no_null_stats['doc_token_length']\n",
    "    no_null_stats['exp_token_length_frac'] = no_null_stats['exp_doc_length']/ no_null_stats['exp_token_length']\n",
    "\n",
    "    return no_null_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aed25140-5c59-4494-8500-194308b047c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the fractional information\n",
    "def pivot_data_by_annotations(df):\n",
    "    pivot_df = df.pivot(index=['doc_id'], columns=['annotation'], values=['token_length_frac', 'exp_token_length_frac']).reset_index()\n",
    "    print(\"pivot_df.shape\", pivot_df.shape)\n",
    "    display(pivot_df.isna().sum().reset_index().rename(columns={'index':'column_names', 0:'null_count'}))\n",
    "    pivot_df.fillna(0, inplace=True)\n",
    "    print(f\"\\\\n After replacing NA with 0s\")\n",
    "    display(pivot_df.isna().sum().reset_index().rename(columns={'index':'column_names', 0:'null_count'}))\n",
    "\n",
    "    return pivot_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25da05b-5c55-493f-bd34-6d15ea7c162e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcf6e88b-938e-4793-8314-96ef29bc6b67",
   "metadata": {},
   "source": [
    "### Main eda/ pipeline\n",
    "\n",
    "1. Read the train data file\n",
    "2. Transform the data\n",
    "3. Perform sanity checks\n",
    "4. Save the file locally for further feature transformation\n",
    "5. Find the fractions of token length in each category - find actual fraction and expected fraction\n",
    "6. Pivot the fractional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "189acbc4-f836-4b7c-9515-c03c7f82641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26414, 13)\n",
      "CPU times: user 907 ms, sys: 1.19 s, total: 2.1 s\n",
      "Wall time: 5.49 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_label_ids</th>\n",
       "      <th>annotation</th>\n",
       "      <th>start_id</th>\n",
       "      <th>end_id</th>\n",
       "      <th>token_length</th>\n",
       "      <th>token_label_id_length</th>\n",
       "      <th>exp_token_length</th>\n",
       "      <th>doc_token_length</th>\n",
       "      <th>len_unique_token_ids</th>\n",
       "      <th>unique_token_id</th>\n",
       "      <th>expected_token_label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15096</td>\n",
       "      <td>[Across, the, world,, Emergency, Departments, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>3779</td>\n",
       "      <td>3779</td>\n",
       "      <td>3779</td>\n",
       "      <td>3779</td>\n",
       "      <td>3821</td>\n",
       "      <td>3</td>\n",
       "      <td>{0, 1, 3}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15096</td>\n",
       "      <td>[Resources,, Project, administration,, Supervi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>3780</td>\n",
       "      <td>7601</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>3821</td>\n",
       "      <td>3821</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15096</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>summarized</td>\n",
       "      <td>7602</td>\n",
       "      <td>9401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1799</td>\n",
       "      <td>3821</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15096</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>human</td>\n",
       "      <td>9402</td>\n",
       "      <td>25014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15612</td>\n",
       "      <td>3821</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14428</td>\n",
       "      <td>[lung, Crab, is, the, in, the, lead, make, of,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>0</td>\n",
       "      <td>4166</td>\n",
       "      <td>4166</td>\n",
       "      <td>4166</td>\n",
       "      <td>4166</td>\n",
       "      <td>4330</td>\n",
       "      <td>2</td>\n",
       "      <td>{0, 1}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                             tokens  \\\n",
       "0   15096  [Across, the, world,, Emergency, Departments, ...   \n",
       "1   15096  [Resources,, Project, administration,, Supervi...   \n",
       "2   15096                                               None   \n",
       "3   15096                                               None   \n",
       "4   14428  [lung, Crab, is, the, in, the, lead, make, of,...   \n",
       "\n",
       "                                     token_label_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                 annotation  start_id  end_id  token_length  \\\n",
       "0                     human         0    3779          3779   \n",
       "1  NLTK_synonym_replacement      3780    7601            41   \n",
       "2                summarized      7602    9401             0   \n",
       "3                     human      9402   25014             0   \n",
       "4  NLTK_synonym_replacement         0    4166          4166   \n",
       "\n",
       "   token_label_id_length  exp_token_length  doc_token_length  \\\n",
       "0                   3779              3779              3821   \n",
       "1                     41              3821              3821   \n",
       "2                      0              1799              3821   \n",
       "3                      0             15612              3821   \n",
       "4                   4166              4166              4330   \n",
       "\n",
       "   len_unique_token_ids unique_token_id  expected_token_label_id  \n",
       "0                     3       {0, 1, 3}                        0  \n",
       "1                     1               0                        1  \n",
       "2                     0            None                        3  \n",
       "3                     0            None                        0  \n",
       "4                     2          {0, 1}                        1  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Read the train data file\n",
    "\n",
    "# Transform the data\n",
    "transformed_train_df = split_text_rows_on_annotation(train_df)\n",
    "print(transformed_train_df.shape)\n",
    "transformed_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bd27c533-9e51-4d35-a7e5-c2143e10c0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in the df (26414, 13)\n",
      "\n",
      "Null check\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_names</th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_id</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tokens</td>\n",
       "      <td>18630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_label_ids</td>\n",
       "      <td>18630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>annotation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>start_id</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>end_id</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>token_length</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>token_label_id_length</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>exp_token_length</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_token_length</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>len_unique_token_ids</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>unique_token_id</td>\n",
       "      <td>18630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>expected_token_label_id</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               column_names  null_count\n",
       "0                    doc_id           0\n",
       "1                    tokens       18630\n",
       "2           token_label_ids       18630\n",
       "3                annotation           0\n",
       "4                  start_id           0\n",
       "5                    end_id           0\n",
       "6              token_length           0\n",
       "7     token_label_id_length           0\n",
       "8          exp_token_length           0\n",
       "9          doc_token_length           0\n",
       "10     len_unique_token_ids           0\n",
       "11          unique_token_id       18630\n",
       "12  expected_token_label_id           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mismatches where tokens and token_label_ids in input file do not have the same size\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_label_ids</th>\n",
       "      <th>annotation</th>\n",
       "      <th>start_id</th>\n",
       "      <th>end_id</th>\n",
       "      <th>token_length</th>\n",
       "      <th>token_label_id_length</th>\n",
       "      <th>exp_token_length</th>\n",
       "      <th>doc_token_length</th>\n",
       "      <th>len_unique_token_ids</th>\n",
       "      <th>unique_token_id</th>\n",
       "      <th>expected_token_label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [doc_id, tokens, token_label_ids, annotation, start_id, end_id, token_length, token_label_id_length, exp_token_length, doc_token_length, len_unique_token_ids, unique_token_id, expected_token_label_id]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows with more than one unique token id 6588\n",
      "\n",
      "Number of rows with only one unique token id 1196\n",
      "Rows with only one unique token id but the token id does not match with the expected token label id\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_label_ids</th>\n",
       "      <th>annotation</th>\n",
       "      <th>start_id</th>\n",
       "      <th>end_id</th>\n",
       "      <th>token_length</th>\n",
       "      <th>token_label_id_length</th>\n",
       "      <th>exp_token_length</th>\n",
       "      <th>doc_token_length</th>\n",
       "      <th>len_unique_token_ids</th>\n",
       "      <th>unique_token_id</th>\n",
       "      <th>expected_token_label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15096</td>\n",
       "      <td>[Resources,, Project, administration,, Supervi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>3780</td>\n",
       "      <td>7601</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>3821</td>\n",
       "      <td>3821</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2144</td>\n",
       "      <td>[and, PKM, are, receiver, of, a, Ministry, of,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>human</td>\n",
       "      <td>3265</td>\n",
       "      <td>17179</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>13914</td>\n",
       "      <td>3439</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5826</td>\n",
       "      <td>[and, thus, combat, vaccine, hesitancy,, parti...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>3667</td>\n",
       "      <td>6954</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>3287</td>\n",
       "      <td>3819</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>24410</td>\n",
       "      <td>[last, two, years,, we, have, been, .»»»., ., ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>human</td>\n",
       "      <td>6331</td>\n",
       "      <td>19986</td>\n",
       "      <td>630</td>\n",
       "      <td>630</td>\n",
       "      <td>13655</td>\n",
       "      <td>6961</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>8525</td>\n",
       "      <td>[temporal, envelope, vectors, of, the, target,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>summarized</td>\n",
       "      <td>2692</td>\n",
       "      <td>4557</td>\n",
       "      <td>1865</td>\n",
       "      <td>1865</td>\n",
       "      <td>1865</td>\n",
       "      <td>9056</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26363</th>\n",
       "      <td>14116</td>\n",
       "      <td>[Mo, were, enriched, in, the, austenite, grain...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>summarized</td>\n",
       "      <td>2870</td>\n",
       "      <td>4148</td>\n",
       "      <td>1278</td>\n",
       "      <td>1278</td>\n",
       "      <td>1278</td>\n",
       "      <td>5916</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26364</th>\n",
       "      <td>14116</td>\n",
       "      <td>[and, ultimate, tensile, strength, of, 996, MP...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>4149</td>\n",
       "      <td>6123</td>\n",
       "      <td>1767</td>\n",
       "      <td>1767</td>\n",
       "      <td>1974</td>\n",
       "      <td>5916</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26369</th>\n",
       "      <td>22258</td>\n",
       "      <td>[in, 1, H,, 15, N, amide, shifts, less, than, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>2816</td>\n",
       "      <td>6546</td>\n",
       "      <td>3730</td>\n",
       "      <td>3730</td>\n",
       "      <td>3730</td>\n",
       "      <td>7784</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26395</th>\n",
       "      <td>13123</td>\n",
       "      <td>[from, soybean, (, Fig., 2, )., This, is, expl...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>summarized</td>\n",
       "      <td>2824</td>\n",
       "      <td>4013</td>\n",
       "      <td>1189</td>\n",
       "      <td>1189</td>\n",
       "      <td>1189</td>\n",
       "      <td>7300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26403</th>\n",
       "      <td>19648</td>\n",
       "      <td>[(4), S, =, {, q, 0, }, and, Φ, current, =, ∅,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>5518</td>\n",
       "      <td>8353</td>\n",
       "      <td>2835</td>\n",
       "      <td>2835</td>\n",
       "      <td>2835</td>\n",
       "      <td>9314</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id                                             tokens  \\\n",
       "1       15096  [Resources,, Project, administration,, Supervi...   \n",
       "7        2144  [and, PKM, are, receiver, of, a, Ministry, of,...   \n",
       "10       5826  [and, thus, combat, vaccine, hesitancy,, parti...   \n",
       "82      24410  [last, two, years,, we, have, been, .»»»., ., ...   \n",
       "89       8525  [temporal, envelope, vectors, of, the, target,...   \n",
       "...       ...                                                ...   \n",
       "26363   14116  [Mo, were, enriched, in, the, austenite, grain...   \n",
       "26364   14116  [and, ultimate, tensile, strength, of, 996, MP...   \n",
       "26369   22258  [in, 1, H,, 15, N, amide, shifts, less, than, ...   \n",
       "26395   13123  [from, soybean, (, Fig., 2, )., This, is, expl...   \n",
       "26403   19648  [(4), S, =, {, q, 0, }, and, Φ, current, =, ∅,...   \n",
       "\n",
       "                                         token_label_ids  \\\n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "10     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "82     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "89     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "26363  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26364  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26369  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26395  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26403  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                     annotation  start_id  end_id  token_length  \\\n",
       "1      NLTK_synonym_replacement      3780    7601            41   \n",
       "7                         human      3265   17179           174   \n",
       "10                      chatgpt      3667    6954           152   \n",
       "82                        human      6331   19986           630   \n",
       "89                   summarized      2692    4557          1865   \n",
       "...                         ...       ...     ...           ...   \n",
       "26363                summarized      2870    4148          1278   \n",
       "26364                   chatgpt      4149    6123          1767   \n",
       "26369  NLTK_synonym_replacement      2816    6546          3730   \n",
       "26395                summarized      2824    4013          1189   \n",
       "26403                   chatgpt      5518    8353          2835   \n",
       "\n",
       "       token_label_id_length  exp_token_length  doc_token_length  \\\n",
       "1                         41              3821              3821   \n",
       "7                        174             13914              3439   \n",
       "10                       152              3287              3819   \n",
       "82                       630             13655              6961   \n",
       "89                      1865              1865              9056   \n",
       "...                      ...               ...               ...   \n",
       "26363                   1278              1278              5916   \n",
       "26364                   1767              1974              5916   \n",
       "26369                   3730              3730              7784   \n",
       "26395                   1189              1189              7300   \n",
       "26403                   2835              2835              9314   \n",
       "\n",
       "       len_unique_token_ids unique_token_id  expected_token_label_id  \n",
       "1                         1               0                        1  \n",
       "7                         1               1                        0  \n",
       "10                        1               0                        2  \n",
       "82                        1               3                        0  \n",
       "89                        1               0                        3  \n",
       "...                     ...             ...                      ...  \n",
       "26363                     1               0                        3  \n",
       "26364                     1               0                        2  \n",
       "26369                     1               0                        1  \n",
       "26395                     1               0                        3  \n",
       "26403                     1               0                        2  \n",
       "\n",
       "[543 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution for the data with unique labels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "      <th>expected_token_label_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summarized</td>\n",
       "      <td>3</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 annotation  expected_token_label_id  count\n",
       "0                     human                        0    663\n",
       "1                summarized                        3    192\n",
       "2                   chatgpt                        2    191\n",
       "3  NLTK_synonym_replacement                        1    150"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution for all the data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "      <th>expected_token_label_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>13346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>2</td>\n",
       "      <td>4447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summarized</td>\n",
       "      <td>3</td>\n",
       "      <td>4376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>1</td>\n",
       "      <td>4245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 annotation  expected_token_label_id  count\n",
       "0                     human                        0  13346\n",
       "1                   chatgpt                        2   4447\n",
       "2                summarized                        3   4376\n",
       "3  NLTK_synonym_replacement                        1   4245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token id distribution for unique labels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "      <th>unique_token_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summarized</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>human</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>human</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>summarized</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>summarized</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>summarized</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  annotation  unique_token_id  count\n",
       "0                      human                0    645\n",
       "1                    chatgpt                0    184\n",
       "2                 summarized                0    184\n",
       "3   NLTK_synonym_replacement                0    142\n",
       "4                      human                1      9\n",
       "5                    chatgpt                1      5\n",
       "6                      human                2      5\n",
       "7   NLTK_synonym_replacement                1      4\n",
       "8   NLTK_synonym_replacement                3      4\n",
       "9                      human                3      4\n",
       "10                summarized                1      3\n",
       "11                summarized                2      3\n",
       "12                   chatgpt                2      2\n",
       "13                summarized                3      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform sanity checks\n",
    "sanity_checks(transformed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6226c6-4d9b-4210-b2cd-d39e64e95c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3cdf3104-0ed4-4523-a4e9-02184e1f891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save the file locally for further feature transformation\n",
    "transformed_train_df.to_csv('../data/tranformed_train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "882c899e-c120-46de-8051-8795f4967f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7405, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>expected_token_label_id</th>\n",
       "      <th>doc_token_length</th>\n",
       "      <th>token_label_id_length</th>\n",
       "      <th>exp_token_length</th>\n",
       "      <th>end_id</th>\n",
       "      <th>exp_doc_length</th>\n",
       "      <th>token_length_frac</th>\n",
       "      <th>exp_token_length_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>6604</td>\n",
       "      <td>4979</td>\n",
       "      <td>8292</td>\n",
       "      <td>9917</td>\n",
       "      <td>9918</td>\n",
       "      <td>0.753937</td>\n",
       "      <td>1.196093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>summarized</td>\n",
       "      <td>3</td>\n",
       "      <td>6604</td>\n",
       "      <td>1624</td>\n",
       "      <td>1624</td>\n",
       "      <td>1624</td>\n",
       "      <td>1625</td>\n",
       "      <td>0.245912</td>\n",
       "      <td>1.000616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>6099</td>\n",
       "      <td>6099</td>\n",
       "      <td>19153</td>\n",
       "      <td>19153</td>\n",
       "      <td>19154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>3118</td>\n",
       "      <td>3118</td>\n",
       "      <td>7747</td>\n",
       "      <td>7747</td>\n",
       "      <td>7748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>3847</td>\n",
       "      <td>2249</td>\n",
       "      <td>3503</td>\n",
       "      <td>5101</td>\n",
       "      <td>5102</td>\n",
       "      <td>0.584611</td>\n",
       "      <td>1.456466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  annotation  expected_token_label_id  doc_token_length  \\\n",
       "0      10       human                        0              6604   \n",
       "1      10  summarized                        3              6604   \n",
       "2      13       human                        0              6099   \n",
       "3      19       human                        0              3118   \n",
       "4      28       human                        0              3847   \n",
       "\n",
       "   token_label_id_length  exp_token_length  end_id  exp_doc_length  \\\n",
       "0                   4979              8292    9917            9918   \n",
       "1                   1624              1624    1624            1625   \n",
       "2                   6099             19153   19153           19154   \n",
       "3                   3118              7747    7747            7748   \n",
       "4                   2249              3503    5101            5102   \n",
       "\n",
       "   token_length_frac  exp_token_length_frac  \n",
       "0           0.753937               1.196093  \n",
       "1           0.245912               1.000616  \n",
       "2           1.000000               1.000052  \n",
       "3           1.000000               1.000129  \n",
       "4           0.584611               1.456466  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the fractions of token length in each category - find actual fraction and expected fraction\n",
    "fractions = find_fractional_length(transformed_train_df)\n",
    "print(fractions.shape)\n",
    "fractions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d6773e36-9bec-4c34-b4e0-cbe1f231c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pivot_df.shape (5000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>annotation</th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_id</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_length_frac</td>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>4145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_length_frac</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>4092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>token_length_frac</td>\n",
       "      <td>human</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>token_length_frac</td>\n",
       "      <td>summarized</td>\n",
       "      <td>4133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>exp_token_length_frac</td>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>4145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>exp_token_length_frac</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>4092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>exp_token_length_frac</td>\n",
       "      <td>human</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>exp_token_length_frac</td>\n",
       "      <td>summarized</td>\n",
       "      <td>4133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 level_0                annotation  null_count\n",
       "0                 doc_id                                     0\n",
       "1      token_length_frac  NLTK_synonym_replacement        4145\n",
       "2      token_length_frac                   chatgpt        4092\n",
       "3      token_length_frac                     human         225\n",
       "4      token_length_frac                summarized        4133\n",
       "5  exp_token_length_frac  NLTK_synonym_replacement        4145\n",
       "6  exp_token_length_frac                   chatgpt        4092\n",
       "7  exp_token_length_frac                     human         225\n",
       "8  exp_token_length_frac                summarized        4133"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n After replacing NA with 0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>annotation</th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_id</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_length_frac</td>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_length_frac</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>token_length_frac</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>token_length_frac</td>\n",
       "      <td>summarized</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>exp_token_length_frac</td>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>exp_token_length_frac</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>exp_token_length_frac</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>exp_token_length_frac</td>\n",
       "      <td>summarized</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 level_0                annotation  null_count\n",
       "0                 doc_id                                     0\n",
       "1      token_length_frac  NLTK_synonym_replacement           0\n",
       "2      token_length_frac                   chatgpt           0\n",
       "3      token_length_frac                     human           0\n",
       "4      token_length_frac                summarized           0\n",
       "5  exp_token_length_frac  NLTK_synonym_replacement           0\n",
       "6  exp_token_length_frac                   chatgpt           0\n",
       "7  exp_token_length_frac                     human           0\n",
       "8  exp_token_length_frac                summarized           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th colspan=\"4\" halign=\"left\">token_length_frac</th>\n",
       "      <th colspan=\"4\" halign=\"left\">exp_token_length_frac</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotation</th>\n",
       "      <th></th>\n",
       "      <th>NLTK_synonym_replacement</th>\n",
       "      <th>chatgpt</th>\n",
       "      <th>human</th>\n",
       "      <th>summarized</th>\n",
       "      <th>NLTK_synonym_replacement</th>\n",
       "      <th>chatgpt</th>\n",
       "      <th>human</th>\n",
       "      <th>summarized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.753937</td>\n",
       "      <td>0.245912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.196093</td>\n",
       "      <td>1.000616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000052</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000129</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.584611</td>\n",
       "      <td>0.415129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.456466</td>\n",
       "      <td>1.000626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_id        token_length_frac                               \\\n",
       "annotation        NLTK_synonym_replacement chatgpt     human summarized   \n",
       "0              10                      0.0     0.0  0.753937   0.245912   \n",
       "1              13                      0.0     0.0  1.000000   0.000000   \n",
       "2              19                      0.0     0.0  1.000000   0.000000   \n",
       "3              28                      0.0     0.0  0.584611   0.415129   \n",
       "4              40                      0.0     0.0  1.000000   0.000000   \n",
       "\n",
       "              exp_token_length_frac                               \n",
       "annotation NLTK_synonym_replacement chatgpt     human summarized  \n",
       "0                               0.0     0.0  1.196093   1.000616  \n",
       "1                               0.0     0.0  1.000052   0.000000  \n",
       "2                               0.0     0.0  1.000129   0.000000  \n",
       "3                               0.0     0.0  1.456466   1.000626  \n",
       "4                               0.0     0.0  1.000040   0.000000  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivot the fractional information\n",
    "pivot_df = pivot_data_by_annotations(fractions)\n",
    "pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6db9ef-4e99-43b5-b58b-74a0516d06d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb3961-d012-4b54-94c1-098782c10541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643744f-b915-480c-bd56-222e78cb0996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d4fa5-7b24-46dc-b6ae-4c8640d65d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc78af2-6c4c-4f83-8b59-2bca00869540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26a49376-2069-48c6-be68-8cf42b91913a",
   "metadata": {},
   "source": [
    "### Scratch pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fb32b277-70ac-4000-a218-597af0df7b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7784, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_label_ids</th>\n",
       "      <th>annotation</th>\n",
       "      <th>start_id</th>\n",
       "      <th>end_id</th>\n",
       "      <th>token_length</th>\n",
       "      <th>token_label_id_length</th>\n",
       "      <th>exp_token_length</th>\n",
       "      <th>doc_token_length</th>\n",
       "      <th>len_unique_token_ids</th>\n",
       "      <th>unique_token_id</th>\n",
       "      <th>expected_token_label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15096</td>\n",
       "      <td>[Across, the, world,, Emergency, Departments, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>3779</td>\n",
       "      <td>3779</td>\n",
       "      <td>3779</td>\n",
       "      <td>3779</td>\n",
       "      <td>3821</td>\n",
       "      <td>3</td>\n",
       "      <td>{0, 1, 3}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15096</td>\n",
       "      <td>[Resources,, Project, administration,, Supervi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>3780</td>\n",
       "      <td>7601</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>3821</td>\n",
       "      <td>3821</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14428</td>\n",
       "      <td>[lung, Crab, is, the, in, the, lead, make, of,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>NLTK_synonym_replacement</td>\n",
       "      <td>0</td>\n",
       "      <td>4166</td>\n",
       "      <td>4166</td>\n",
       "      <td>4166</td>\n",
       "      <td>4166</td>\n",
       "      <td>4330</td>\n",
       "      <td>2</td>\n",
       "      <td>{0, 1}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14428</td>\n",
       "      <td>[after, surgery), is, of, great, importance, f...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>human</td>\n",
       "      <td>4167</td>\n",
       "      <td>27851</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>23684</td>\n",
       "      <td>4330</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2144</td>\n",
       "      <td>[The, number, of, osteoporotic, fractures,, pa...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>0</td>\n",
       "      <td>3264</td>\n",
       "      <td>3264</td>\n",
       "      <td>3264</td>\n",
       "      <td>3264</td>\n",
       "      <td>3439</td>\n",
       "      <td>3</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                             tokens  \\\n",
       "0   15096  [Across, the, world,, Emergency, Departments, ...   \n",
       "1   15096  [Resources,, Project, administration,, Supervi...   \n",
       "2   14428  [lung, Crab, is, the, in, the, lead, make, of,...   \n",
       "3   14428  [after, surgery), is, of, great, importance, f...   \n",
       "4    2144  [The, number, of, osteoporotic, fractures,, pa...   \n",
       "\n",
       "                                     token_label_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                 annotation  start_id  end_id  token_length  \\\n",
       "0                     human         0    3779          3779   \n",
       "1  NLTK_synonym_replacement      3780    7601            41   \n",
       "2  NLTK_synonym_replacement         0    4166          4166   \n",
       "3                     human      4167   27851           163   \n",
       "4                   chatgpt         0    3264          3264   \n",
       "\n",
       "   token_label_id_length  exp_token_length  doc_token_length  \\\n",
       "0                   3779              3779              3821   \n",
       "1                     41              3821              3821   \n",
       "2                   4166              4166              4330   \n",
       "3                    163             23684              4330   \n",
       "4                   3264              3264              3439   \n",
       "\n",
       "   len_unique_token_ids unique_token_id  expected_token_label_id  \n",
       "0                     3       {0, 1, 3}                        0  \n",
       "1                     1               0                        1  \n",
       "2                     2          {0, 1}                        1  \n",
       "3                     1               0                        0  \n",
       "4                     3       {0, 1, 2}                        2  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_null_data = transformed_train_df.dropna(subset=['tokens'], ignore_index=True)\n",
    "print(no_null_data.shape)\n",
    "no_null_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "893da842-90de-4517-9777-300e9d138411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>expected_token_label_id</th>\n",
       "      <th>doc_token_length</th>\n",
       "      <th>end_id</th>\n",
       "      <th>token_label_id_length</th>\n",
       "      <th>exp_token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>6604</td>\n",
       "      <td>9917</td>\n",
       "      <td>4979</td>\n",
       "      <td>8292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>summarized</td>\n",
       "      <td>3</td>\n",
       "      <td>6604</td>\n",
       "      <td>1624</td>\n",
       "      <td>1624</td>\n",
       "      <td>1624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>6099</td>\n",
       "      <td>19153</td>\n",
       "      <td>6099</td>\n",
       "      <td>19153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>3118</td>\n",
       "      <td>7747</td>\n",
       "      <td>3118</td>\n",
       "      <td>7747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>3847</td>\n",
       "      <td>5101</td>\n",
       "      <td>2249</td>\n",
       "      <td>3503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7779</th>\n",
       "      <td>24984</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>6913</td>\n",
       "      <td>8416</td>\n",
       "      <td>1796</td>\n",
       "      <td>3299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7780</th>\n",
       "      <td>24984</td>\n",
       "      <td>summarized</td>\n",
       "      <td>3</td>\n",
       "      <td>6913</td>\n",
       "      <td>5116</td>\n",
       "      <td>1254</td>\n",
       "      <td>1254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7781</th>\n",
       "      <td>24991</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>9803</td>\n",
       "      <td>50049</td>\n",
       "      <td>9803</td>\n",
       "      <td>50049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7782</th>\n",
       "      <td>24993</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>2</td>\n",
       "      <td>6241</td>\n",
       "      <td>2783</td>\n",
       "      <td>2783</td>\n",
       "      <td>2783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7783</th>\n",
       "      <td>24993</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>6241</td>\n",
       "      <td>9303</td>\n",
       "      <td>3457</td>\n",
       "      <td>6519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7784 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc_id  annotation  expected_token_label_id  doc_token_length  end_id  \\\n",
       "0         10       human                        0              6604    9917   \n",
       "1         10  summarized                        3              6604    1624   \n",
       "2         13       human                        0              6099   19153   \n",
       "3         19       human                        0              3118    7747   \n",
       "4         28       human                        0              3847    5101   \n",
       "...      ...         ...                      ...               ...     ...   \n",
       "7779   24984       human                        0              6913    8416   \n",
       "7780   24984  summarized                        3              6913    5116   \n",
       "7781   24991       human                        0              9803   50049   \n",
       "7782   24993     chatgpt                        2              6241    2783   \n",
       "7783   24993       human                        0              6241    9303   \n",
       "\n",
       "      token_label_id_length  exp_token_length  \n",
       "0                      4979              8292  \n",
       "1                      1624              1624  \n",
       "2                      6099             19153  \n",
       "3                      3118              7747  \n",
       "4                      2249              3503  \n",
       "...                     ...               ...  \n",
       "7779                   1796              3299  \n",
       "7780                   1254              1254  \n",
       "7781                   9803             50049  \n",
       "7782                   2783              2783  \n",
       "7783                   3457              6519  \n",
       "\n",
       "[7784 rows x 7 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_null_data.groupby(['doc_id', 'annotation', 'expected_token_label_id', 'doc_token_length', 'end_id'])[['token_label_id_length', 'exp_token_length']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a88cad6b-928e-4977-8d95-d28d025f2af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7405, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>expected_token_label_id</th>\n",
       "      <th>doc_token_length</th>\n",
       "      <th>token_label_id_length</th>\n",
       "      <th>exp_token_length</th>\n",
       "      <th>end_id</th>\n",
       "      <th>exp_doc_length</th>\n",
       "      <th>token_length_frac</th>\n",
       "      <th>exp_token_length_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>6604</td>\n",
       "      <td>4979</td>\n",
       "      <td>8292</td>\n",
       "      <td>9917</td>\n",
       "      <td>9918</td>\n",
       "      <td>0.753937</td>\n",
       "      <td>1.196093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>summarized</td>\n",
       "      <td>3</td>\n",
       "      <td>6604</td>\n",
       "      <td>1624</td>\n",
       "      <td>1624</td>\n",
       "      <td>1624</td>\n",
       "      <td>1625</td>\n",
       "      <td>0.245912</td>\n",
       "      <td>1.000616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>6099</td>\n",
       "      <td>6099</td>\n",
       "      <td>19153</td>\n",
       "      <td>19153</td>\n",
       "      <td>19154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>3118</td>\n",
       "      <td>3118</td>\n",
       "      <td>7747</td>\n",
       "      <td>7747</td>\n",
       "      <td>7748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>3847</td>\n",
       "      <td>2249</td>\n",
       "      <td>3503</td>\n",
       "      <td>5101</td>\n",
       "      <td>5102</td>\n",
       "      <td>0.584611</td>\n",
       "      <td>1.456466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  annotation  expected_token_label_id  doc_token_length  \\\n",
       "0      10       human                        0              6604   \n",
       "1      10  summarized                        3              6604   \n",
       "2      13       human                        0              6099   \n",
       "3      19       human                        0              3118   \n",
       "4      28       human                        0              3847   \n",
       "\n",
       "   token_label_id_length  exp_token_length  end_id  exp_doc_length  \\\n",
       "0                   4979              8292    9917            9918   \n",
       "1                   1624              1624    1624            1625   \n",
       "2                   6099             19153   19153           19154   \n",
       "3                   3118              7747    7747            7748   \n",
       "4                   2249              3503    5101            5102   \n",
       "\n",
       "   token_length_frac  exp_token_length_frac  \n",
       "0           0.753937               1.196093  \n",
       "1           0.245912               1.000616  \n",
       "2           1.000000               1.000052  \n",
       "3           1.000000               1.000129  \n",
       "4           0.584611               1.456466  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractrions = find_fractional_length(no_null_data)\n",
    "print(fractrions.shape)\n",
    "fractrions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4df9a2ca-04b9-43ff-8a12-f7c1a02e3d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th colspan=\"4\" halign=\"left\">token_length_frac</th>\n",
       "      <th colspan=\"4\" halign=\"left\">exp_token_length_frac</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotation</th>\n",
       "      <th></th>\n",
       "      <th>NLTK_synonym_replacement</th>\n",
       "      <th>chatgpt</th>\n",
       "      <th>human</th>\n",
       "      <th>summarized</th>\n",
       "      <th>NLTK_synonym_replacement</th>\n",
       "      <th>chatgpt</th>\n",
       "      <th>human</th>\n",
       "      <th>summarized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.753937</td>\n",
       "      <td>0.245912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.196093</td>\n",
       "      <td>1.000616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000052</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000129</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.584611</td>\n",
       "      <td>0.415129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.456466</td>\n",
       "      <td>1.000626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>24975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000253</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>24977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509058</td>\n",
       "      <td>0.490791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.925726</td>\n",
       "      <td>1.000308</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>24984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.818313</td>\n",
       "      <td>0.181397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.175559</td>\n",
       "      <td>4.080542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>24991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>24993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.445922</td>\n",
       "      <td>0.553918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000359</td>\n",
       "      <td>1.427213</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_id        token_length_frac                                 \\\n",
       "annotation        NLTK_synonym_replacement   chatgpt     human summarized   \n",
       "0              10                      NaN       NaN  0.753937   0.245912   \n",
       "1              13                      NaN       NaN  1.000000        NaN   \n",
       "2              19                      NaN       NaN  1.000000        NaN   \n",
       "3              28                      NaN       NaN  0.584611   0.415129   \n",
       "4              40                      NaN       NaN  1.000000        NaN   \n",
       "...           ...                      ...       ...       ...        ...   \n",
       "4995        24975                      NaN       NaN  1.000000        NaN   \n",
       "4996        24977                      NaN  0.509058  0.490791        NaN   \n",
       "4997        24984                      NaN       NaN  0.818313   0.181397   \n",
       "4998        24991                      NaN       NaN  1.000000        NaN   \n",
       "4999        24993                      NaN  0.445922  0.553918        NaN   \n",
       "\n",
       "              exp_token_length_frac                                 \n",
       "annotation NLTK_synonym_replacement   chatgpt     human summarized  \n",
       "0                               NaN       NaN  1.196093   1.000616  \n",
       "1                               NaN       NaN  1.000052        NaN  \n",
       "2                               NaN       NaN  1.000129        NaN  \n",
       "3                               NaN       NaN  1.456466   1.000626  \n",
       "4                               NaN       NaN  1.000040        NaN  \n",
       "...                             ...       ...       ...        ...  \n",
       "4995                            NaN       NaN  1.000253        NaN  \n",
       "4996                            NaN  1.925726  1.000308        NaN  \n",
       "4997                            NaN       NaN  1.175559   4.080542  \n",
       "4998                            NaN       NaN  1.000020        NaN  \n",
       "4999                            NaN  1.000359  1.427213        NaN  \n",
       "\n",
       "[5000 rows x 9 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractrions.pivot(index=['doc_id'], columns=['annotation'], values=['token_length_frac', 'exp_token_length_frac']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "341f67d2-e669-405f-b135-df9da57f6107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc_id  annotation\n",
       "10      human         1\n",
       "        summarized    1\n",
       "13      human         1\n",
       "19      human         1\n",
       "28      human         1\n",
       "                     ..\n",
       "24984   human         2\n",
       "        summarized    1\n",
       "24991   human         1\n",
       "24993   chatgpt       1\n",
       "        human         1\n",
       "Name: count, Length: 7405, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractrions.groupby(['doc_id', 'annotation'])['doc_id'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49036ac-f30b-4300-9ad1-eaed4777a0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu121.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m119"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
